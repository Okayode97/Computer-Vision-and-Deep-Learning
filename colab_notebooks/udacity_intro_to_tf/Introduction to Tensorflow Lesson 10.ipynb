{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Introduction to Tensorflow Lesson 10.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP+87Ea43LWHLKmLP+n8AK+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Lesson 10: NLP Recurrent Neural Networks**\n","\n","This lesson is split into 2 parts\n","- Introducing recurrent neural networks and other methods and layers which consider seqeunces of data.\n","- Generative text models."],"metadata":{"id":"msdtD9bUyLwT"}},{"cell_type":"markdown","source":["## **Lesson Setup**"],"metadata":{"id":"GwIJjSwkXi8T"}},{"cell_type":"markdown","source":["import lesson dependencies"],"metadata":{"id":"nMIJsufLXxfJ"}},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import tensorflow_datasets as tfds\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","print(tf.__version__)\n","print(np.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n-4jW3iaXinC","executionInfo":{"status":"ok","timestamp":1647983854349,"user_tz":0,"elapsed":614,"user":{"displayName":"Kayode Sonaike","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14696411076652260026"}},"outputId":"46585008-f65a-447f-dfb8-65364c8eed20"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["2.8.0\n","1.21.5\n"]}]},{"cell_type":"markdown","source":["download the text data needed for the review"],"metadata":{"id":"h9N0Hcb2X1Mu"}},{"cell_type":"code","source":["# download the csv file\n","!wget --no-check-certificate \\\n","    -O /tmp/sentiment.csv https://drive.google.com/uc?id=13ySLC_ue6Umt9RJYSeM2t-V0kCv-4C-P"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jFnL_vdZX0oP","executionInfo":{"status":"ok","timestamp":1647984198701,"user_tz":0,"elapsed":984,"user":{"displayName":"Kayode Sonaike","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14696411076652260026"}},"outputId":"03b56158-c36e-474c-9f1d-d045432659d3"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-03-22 21:23:16--  https://drive.google.com/uc?id=13ySLC_ue6Umt9RJYSeM2t-V0kCv-4C-P\n","Resolving drive.google.com (drive.google.com)... 74.125.137.100, 74.125.137.101, 74.125.137.139, ...\n","Connecting to drive.google.com (drive.google.com)|74.125.137.100|:443... connected.\n","HTTP request sent, awaiting response... 303 See Other\n","Location: https://doc-08-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/2iuk9bqircsjtr7337e72sgcocl9m03b/1647984150000/11118900490791463723/*/13ySLC_ue6Umt9RJYSeM2t-V0kCv-4C-P [following]\n","Warning: wildcards not supported in HTTP.\n","--2022-03-22 21:23:17--  https://doc-08-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/2iuk9bqircsjtr7337e72sgcocl9m03b/1647984150000/11118900490791463723/*/13ySLC_ue6Umt9RJYSeM2t-V0kCv-4C-P\n","Resolving doc-08-ak-docs.googleusercontent.com (doc-08-ak-docs.googleusercontent.com)... 142.250.141.132, 2607:f8b0:4023:c0b::84\n","Connecting to doc-08-ak-docs.googleusercontent.com (doc-08-ak-docs.googleusercontent.com)|142.250.141.132|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 127831 (125K) [text/csv]\n","Saving to: ‘/tmp/sentiment.csv’\n","\n","/tmp/sentiment.csv  100%[===================>] 124.83K  --.-KB/s    in 0.08s   \n","\n","2022-03-22 21:23:17 (1.60 MB/s) - ‘/tmp/sentiment.csv’ saved [127831/127831]\n","\n"]}]},{"cell_type":"code","source":["# read the csv file as a pandas dataset\n","dataset = pd.read_csv('/tmp/sentiment.csv')\n","\n","# get the sentences and labels\n","sentences = dataset['text'].tolist()\n","labels = dataset['sentiment'].tolist()\n","\n","# Split the sentences and label into training and test set\n","\n","training_size = int(len(sentences)*0.8)\n","\n","# define training data\n","training_sentences = sentences[:training_size]\n","training_label = labels[:training_size]\n","\n","# define test data\n","testing_sentences = sentences[training_size:]\n","testing_labels = labels[training_size:]\n"],"metadata":{"id":"sf6o7TorZLRF","executionInfo":{"status":"ok","timestamp":1647984217970,"user_tz":0,"elapsed":378,"user":{"displayName":"Kayode Sonaike","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14696411076652260026"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["define some parameters and helper functions"],"metadata":{"id":"TXVshp1_acqI"}},{"cell_type":"code","source":["VOCAB_SIZE = 1000\n","OOV =\"<00V>\"\n","MAX_SEQUENCE_LENGTH = 100\n","MAX_SUBWORD_LENGTH = 5\n"],"metadata":{"id":"7sNJ5-HzacZX","executionInfo":{"status":"ok","timestamp":1647984467947,"user_tz":0,"elapsed":248,"user":{"displayName":"Kayode Sonaike","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14696411076652260026"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["def apply_padding(sequences):\n","  return pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH,\n","                       padding='post', truncating='post')\n","\n","# Before the test data is passed to the model it would have to be prepared.\n","def prepare_test_data(test_sentences, tokenizer):\n","    sequences = tokenizer.texts_to_sequences(testing_sentences)\n","    padded_sequences = apply_padding(sequences)\n","    padded_sequences = np.array(padded_sequences).reshape(-1, MAX_SEQUENCE_LENGTH, len(padded_sequences))\n","    return padded_sequences\n"],"metadata":{"id":"kDkxlmbih30r","executionInfo":{"status":"ok","timestamp":1647985812009,"user_tz":0,"elapsed":254,"user":{"displayName":"Kayode Sonaike","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14696411076652260026"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["Create a padded tokenized sequence"],"metadata":{"id":"s6sC98YsYplz"}},{"cell_type":"code","source":["# create a tokenizer \n","Full_word_tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=VOCAB_SIZE, oov_token=OOV)\n","\n","# fit the tokenizer onto the training sentence\n","Full_word_tokenizer.fit_on_texts(training_sentences)\n","\n","# with the tokenizer convert the training sentences into sequences of tokens\n","Training_sentences_sequences = Full_word_tokenizer.texts_to_sequences(training_sentences)\n","\n","# apply padding to the training sentence sequences\n","Training_sentences_padded_sequences = apply_padding(Training_sentences_sequences)\n"],"metadata":{"id":"ohrt-RDjYpSt","executionInfo":{"status":"ok","timestamp":1647984259814,"user_tz":0,"elapsed":617,"user":{"displayName":"Kayode Sonaike","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14696411076652260026"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Apply the whole process to the testing sentences \n","Testing_sentences_padded_sequences = prepare_test_data(testing_sentences,\n","                                                       Full_word_tokenizer)"],"metadata":{"id":"lKb_6X9lhD6C","executionInfo":{"status":"ok","timestamp":1647984328969,"user_tz":0,"elapsed":4,"user":{"displayName":"Kayode Sonaike","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14696411076652260026"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["Create the subword corpus and tokenizer\n","\n","I'm a bit wary of using this as it has been deprecated. So far for the sake of following the lesson i would keep using subwords."],"metadata":{"id":"M3Twx21UYdvh"}},{"cell_type":"code","source":["# create the subword tokenizer and fit it, to the training sentences\n","subword_tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(training_sentences,\n","                                                                              target_vocab_size = VOCAB_SIZE,\n","                                                                              max_subword_length=MAX_SUBWORD_LENGTH)\n","\n","def create_subword_padded_sequence(sentences):\n","  if type(sentences) == type([]):\n","    tokenized_subword_sequences = []\n","    for sentence in sentences:\n","      tokenized_subword_sequences.append(subword_tokenizer.encode(sentence))\n","    \n","    tokenized_subword_padded_sequences = apply_padding(tokenized_subword_sequences)\n","    \n","\n","  return pass\n"],"metadata":{"id":"DV0LEbZcYfIe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Recurrent Neural Networks**\n","\n","So far in the previous lesson, we have applied\n","- Tokenization to text to convert individual words into unique tokens.\n","- How to generate tokenized sequences from sentences.\n","- Used embeddings to represent these tokens into multi-dimensional vectors which can then be passed to dense layers.\n","- Create subword corpus from text datasets.\n","\n","We have been able to get reasonable good performance with the approach we have taken so far, simply by using embedded tokenized words and subwords. But we have not really considerd the order of the sequence.\n","\n","<br/>\n","\n","Towards this, we would look at **Recurrent neural networks**\n","- These are neural networks that are able to taken in sequences and create an output from the sequence.\n"],"metadata":{"id":"XpIOBrqX02o5"}},{"cell_type":"markdown","source":["### **Significance of considering the order of sequences**\n","\n","Why do we need to consider the order of sequences in our tokenized sequence?\n","- ***Context matters, there are temporal dependencies in our sentences***. The context and quite often the sentiment of our sentence can be determined by certain words. These words can be close or far apart in the text.\n","\n","<br/>\n","\n","For example\n","- i saw a massive _ in the _\n","- i am _ and want to _\n","\n","The 2 sentences above are really ambiguous. But when given more context, we can better predict the missing words and the overall sentiment of the text.\n","- I saw a massive _ in the garden.\n","- I am _ and want to sleep.\n"],"metadata":{"id":"yAfKSOJ8JNIO"}},{"cell_type":"markdown","source":["### **Basics of RNN**\n","- These are neural networks that taken in input sequences and output sequences or vectors. (sequence to sequence or sequence to vector).\n","\n","<br/>\n","\n","Workflow of an RNN\n","- For each value in a sequence, the RNN would use the value as input in each time step to produce an output and a state vector.\n","- The state vector in the last time step is then passed on as an additional input in the next time step.\n","- For the very first time step??? i assume a default value is used for the state vector.\n","\n","<br/>\n","\n","**key point here**   \n","So in this way, some element of the previous input, influences the calculation of the next output & state vector since the previous state vector would be used as an added input.\n","\n","<br/>\n","\n","***That's a very laymans term of covering an RNN without going to deep into it***\n","\n","\n","This lesson focuses on variations of RNN:\n","- Long Short Term Memory (LSTM)"],"metadata":{"id":"xuVe66jSPj4B"}},{"cell_type":"markdown","source":["### **Long Short Term Memory**\n","\n","So we have an high level understanding of how basic RNN work. While it's cool that we are now able to consider our last input basic RNN are not very useful when considering larger bodies of text in which the significant words that influence the overall context and sentiment are farther apart.\n","\n","<br/>\n","\n","So how can we handle temporal dependencies that are really part apart?. LSTMS are our saving grace in this case. These neural networks have a longer short term memory, so in a more naive way it is able to retain information for much longer.\n","\n","<br/>\n","\n","Again not much detail have been documented on these neural networks. I've just tried to keep to the very baiscs.\n","\n","The udacity lesson focused on bi-directional LSTMS, in which information is passed both forwards and backwards through the network."],"metadata":{"id":"c3IFM8zzSS_o"}},{"cell_type":"code","source":["#define a model with a bi-directional LSTM layer\n","\n","simple_lstm_model = tf.keras.Sequential([\n","                             tf.keras.layers.Embedding(VOCAB_SIZE, 64),\n","                             tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n","                             tf.keras.layers.Dense(6, activation='relu'),\n","                             tf.keras.layers.Dense(1, activation='sigmoid')\n","])\n","\n","simple_lstm_model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qoj23-HlPhXK","executionInfo":{"status":"ok","timestamp":1647986243095,"user_tz":0,"elapsed":874,"user":{"displayName":"Kayode Sonaike","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14696411076652260026"}},"outputId":"86aa0a6d-6829-47d0-9ef6-af17beeff332"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, None, 64)          64000     \n","                                                                 \n"," bidirectional (Bidirectiona  (None, 128)              66048     \n"," l)                                                              \n","                                                                 \n"," dense (Dense)               (None, 6)                 774       \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 7         \n","                                                                 \n","=================================================================\n","Total params: 130,829\n","Trainable params: 130,829\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["## **Text Generation**\n","\n"],"metadata":{"id":"-ibPYiOeJFjG"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"riJSWcP5FiUt"},"outputs":[],"source":[""]}]}