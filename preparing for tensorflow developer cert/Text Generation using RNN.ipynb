{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Text Generation using RNN.ipynb","provenance":[],"collapsed_sections":["f2Vk2rqHT-6C","qGEcOjn-EQeD","ye52cOY4FNVr"],"authorship_tag":"ABX9TyNCjIURKUQRbqYQMH23cZ8N"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# **Text generation with an RNN**\n","\n","following [tensorflow tutorial](https://www.tensorflow.org/text/tutorials/text_generation).\n","\n","This tutorial looks at training an RNN model to predict the next character in a sequence. The model is trained on text written by shakespeare.\n","\n","Something to keep in mind. Text generation process demonstrated in Udacity.\n","- Tokenization, followed by converting the texts to sequences.\n","\n","- Each sequence would have a set length. From each sequences, we use the last token as the label and the remaining sequences as the feature vector.\n","\n","- Convert the labels into one-hot vectors, with it's length being the vocabulary size\n","\n","- Train a classification model on the sequences and one-hot labels.\n","\n","- We would then generate text by providing a seed word followed by multiple inference.\n"],"metadata":{"id":"fhLXjY4bLaue"}},{"cell_type":"markdown","source":["##**Import dependencies**"],"metadata":{"id":"yiHsY9gYoTKL"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"rFLuAWMjJUjE","executionInfo":{"status":"ok","timestamp":1661594778708,"user_tz":-60,"elapsed":2986,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"92b7c94e-1b74-4847-bf1a-5c10fb0595ee"},"outputs":[{"output_type":"stream","name":"stdout","text":["2.8.2\n"]}],"source":["import tensorflow as tf\n","import numpy as np\n","import os\n","import time\n","\n","print(tf.__version__)"]},{"cell_type":"markdown","source":["## **Get the dataset**"],"metadata":{"id":"Zm98stqCodZr"}},{"cell_type":"code","source":["path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-cLRkrGcohJ4","executionInfo":{"status":"ok","timestamp":1661594784582,"user_tz":-60,"elapsed":1215,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"a23cc885-2dfa-4e83-bba3-0f0bdee3cf45"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n","1122304/1115394 [==============================] - 0s 0us/step\n","1130496/1115394 [==============================] - 0s 0us/step\n"]}]},{"cell_type":"code","source":["# decode the text\n","text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n","print(text)"],"metadata":{"id":"iCBWlJCppUpm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"Number of characters in the text: {len(text)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hZgnWxdKpsi3","executionInfo":{"status":"ok","timestamp":1661594790703,"user_tz":-60,"elapsed":7,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"14d49abd-f557-4264-85bf-c0738c9d91df"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of characters in the text: 1115394\n"]}]},{"cell_type":"code","source":["# find the number of unique characters in the file\n","vocab = sorted(set(text))\n","print(vocab)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fHTHgiL5p0bX","executionInfo":{"status":"ok","timestamp":1661594848477,"user_tz":-60,"elapsed":332,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"41e0f5ee-e5f7-4324-84a5-adad7f562573"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"]}]},{"cell_type":"code","source":["print(f\"Number of unique charachter in the text is {len(vocab)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AwLD6BSUq1pf","executionInfo":{"status":"ok","timestamp":1661594851969,"user_tz":-60,"elapsed":14,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"f182e924-8d2f-4900-fc34-af3a4763c5b7"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of unique charachter in the text is 65\n"]}]},{"cell_type":"markdown","source":["The approach used here was to download the data as a file, read and then decode it into a list. This is miles better than downloading it  using tensorflow dataset and then loading it.\n","\n","An intresting note there, the vocab size was determined by the number of unique characthers in the dataset. This aligns with the task the model would be trained to do which is to predict the next probable characther given an initial characther.\n"],"metadata":{"id":"cwrpIFmGxMNv"}},{"cell_type":"markdown","source":["## **Process the text**"],"metadata":{"id":"CC4g2LEArRQG"}},{"cell_type":"code","source":["# encode sample string into utf-8 format\n","example_texts = [\"megasxlr\", \"theweekend\"]\n","\n","example_text_chars = tf.strings.unicode_split(example_texts, input_encoding=\"UTF-8\")\n","print(example_text_chars)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sOUDLS05rL8E","executionInfo":{"status":"ok","timestamp":1661594860020,"user_tz":-60,"elapsed":395,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"1e5479c3-4819-415e-9cb7-109032edc30c"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["<tf.RaggedTensor [[b'm', b'e', b'g', b'a', b's', b'x', b'l', b'r'],\n"," [b't', b'h', b'e', b'w', b'e', b'e', b'k', b'e', b'n', b'd']]>\n"]}]},{"cell_type":"code","source":["for index, char_ in enumerate(list(vocab)):\n","  print(f\"index:{index}, character:{char_}\")"],"metadata":{"id":"6P8FD7DZu1y6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Define a text encoder**"],"metadata":{"id":"0ZScqz5b6ig4"}},{"cell_type":"code","source":["# create an encoder to convert the string into token\n","ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab),\n","                                              mask_token=None)\n"],"metadata":{"id":"6WiHkOyduR_d","executionInfo":{"status":"ok","timestamp":1661594883830,"user_tz":-60,"elapsed":232,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["ids = ids_from_chars(example_text_chars)\n","print(ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_30JjnATunoN","executionInfo":{"status":"ok","timestamp":1661594889919,"user_tz":-60,"elapsed":489,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"a148163b-eb40-4341-e243-da726abfb915"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["<tf.RaggedTensor [[52, 44, 46, 40, 58, 63, 51, 57],\n"," [59, 47, 44, 62, 44, 44, 50, 44, 53, 43]]>\n"]}]},{"cell_type":"markdown","source":["it seems like it's encoded the text using the index from the initial vocab (with a slight offset).\n","\n","It looks like [**`tf.keras.layers.StringLookUp`**](https://www.tensorflow.org/api_docs/python/tf/keras/layers/StringLookup) is an alternative to the Tokenizer function which is deprecated in v2.9. The StringLookup is also able to create one-hot vectors to use as tokens for each characthers.\n","\n","Another alternative for tokenization is the [TextVectorization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization)"],"metadata":{"id":"y1G0bLX0valy"}},{"cell_type":"markdown","source":["**Define a text decoder**"],"metadata":{"id":"jrHsi_oS6bm0"}},{"cell_type":"code","source":["# create a text decoder\n","chars_from_ids = tf.keras.layers.StringLookup(vocabulary=ids_from_chars.get_vocabulary(),\n","                                              invert=True,\n","                                              mask_token=None)\n"],"metadata":{"id":"57bIaqxouwDx","executionInfo":{"status":"ok","timestamp":1661594903580,"user_tz":-60,"elapsed":230,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["decoded_chars = chars_from_ids(ids)\n","print(decoded_chars)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TDYqSrLOwGXt","executionInfo":{"status":"ok","timestamp":1661594920055,"user_tz":-60,"elapsed":214,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"36909d92-df6b-48a0-e6d4-667aec97811d"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["<tf.RaggedTensor [[b'm', b'e', b'g', b'a', b's', b'x', b'l', b'r'],\n"," [b't', b'h', b'e', b'w', b'e', b'e', b'k', b'e', b'n', b'd']]>\n"]}]},{"cell_type":"code","source":["# the decoded chars are returns as list of char. We can join the individual\n","# chars back into a string\n","tf.strings.reduce_join(decoded_chars, axis=-1).numpy()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sScbJxSDwf3G","executionInfo":{"status":"ok","timestamp":1661594923201,"user_tz":-60,"elapsed":16,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"533953f4-602e-4003-bdf4-73e9df49b7a2"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([b'megasxlr', b'theweekend'], dtype=object)"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["print(decoded_chars)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Nfo5imlxFIG","executionInfo":{"status":"ok","timestamp":1661594935776,"user_tz":-60,"elapsed":907,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"5dd92dea-2566-4a4b-ec59-663e58703722"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["<tf.RaggedTensor [[b'm', b'e', b'g', b'a', b's', b'x', b'l', b'r'],\n"," [b't', b'h', b'e', b'w', b'e', b'e', b'k', b'e', b'n', b'd']]>\n"]}]},{"cell_type":"code","source":["def text_from_ids(ids):\n","  \"\"\"Function defined to convert string tokens back into an array of strings.\"\"\"\n","  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"],"metadata":{"id":"GOzuCYOC06z0","executionInfo":{"status":"ok","timestamp":1661594938091,"user_tz":-60,"elapsed":2,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["The trained model would be able to predict the next probable character given an initial characther or sequence of characther.\n","\n","Towards this\n","- our training dataset needs to contain text where the input contain parts of the text and label contains the remaining parts of it.\n","- Example: Input: Megas, Label: Megasx\n"],"metadata":{"id":"ZrgT2gB9yRhm"}},{"cell_type":"code","source":["# convert the individual text in the dialog into chars\n","text_split_into_individual_chars = tf.strings.unicode_split(text, 'UTF-8')\n","print(text_split_into_individual_chars)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X9qwEvGD0wpy","executionInfo":{"status":"ok","timestamp":1661594942520,"user_tz":-60,"elapsed":718,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"4cced387-fe73-4591-bf5a-6150c95140af"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor([b'F' b'i' b'r' ... b'g' b'.' b'\\n'], shape=(1115394,), dtype=string)\n"]}]},{"cell_type":"code","source":["# convert each characthers into tokens\n","text_ids = ids_from_chars(text_split_into_individual_chars)\n","print(text_ids)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QBE2QR0n2bCJ","executionInfo":{"status":"ok","timestamp":1661594951186,"user_tz":-60,"elapsed":221,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"30f63c71-761f-43f1-faa0-5f8af7f5b5c8"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor([19 48 57 ... 46  9  1], shape=(1115394,), dtype=int64)\n"]}]},{"cell_type":"code","source":["# generate a tensorsliceDataset from the tensorslice\n","ids_dataset = tf.data.Dataset.from_tensor_slices(text_ids)\n","print(ids_dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IOyGyPYm2uXQ","executionInfo":{"status":"ok","timestamp":1661594957440,"user_tz":-60,"elapsed":227,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"f78afe5e-c7f5-46b2-b3ca-843f60b1d7f6"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["<TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n"]}]},{"cell_type":"code","source":["for ids in ids_dataset.take(10):\n","  print(chars_from_ids(ids).numpy().decode('utf-8'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9WhOEPJO3PKS","executionInfo":{"status":"ok","timestamp":1661594968439,"user_tz":-60,"elapsed":633,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"30a4d2ef-ba86-4a99-c22b-bb9940dae4f4"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["F\n","i\n","r\n","s\n","t\n"," \n","C\n","i\n","t\n","i\n"]}]},{"cell_type":"code","source":["# define the max_length of the sequences\n","seq_length = 100\n"],"metadata":{"id":"SbOSc8Zf3e5k","executionInfo":{"status":"ok","timestamp":1661594975601,"user_tz":-60,"elapsed":1414,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["# from the dataset generate a batch with a length of 101\n","sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n","\n","# display a single batch\n","for seq in sequences.take(5):\n","  print(text_from_ids(seq).numpy())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rIdQDdyS3oev","executionInfo":{"status":"ok","timestamp":1661594977269,"user_tz":-60,"elapsed":10,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"e57bd512-0fcc-426b-edab-8e7a5ae5c45d"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n","b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n","b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n","b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n","b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"]}]},{"cell_type":"code","source":["for seq in sequences.take(5):\n","  print(f\"{seq}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rSCJ1kPV1X33","executionInfo":{"status":"ok","timestamp":1661595042985,"user_tz":-60,"elapsed":836,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"5687d9e4-324b-4ac8-97d5-c54c446b7fcb"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["[19 48 57 58 59  2 16 48 59 48 65 44 53 11  1 15 44 45 54 57 44  2 62 44\n","  2 55 57 54 42 44 44 43  2 40 53 64  2 45 60 57 59 47 44 57  7  2 47 44\n"," 40 57  2 52 44  2 58 55 44 40 50  9  1  1 14 51 51 11  1 32 55 44 40 50\n","  7  2 58 55 44 40 50  9  1  1 19 48 57 58 59  2 16 48 59 48 65 44 53 11\n","  1 38 54 60  2]\n","[40 57 44  2 40 51 51  2 57 44 58 54 51 61 44 43  2 57 40 59 47 44 57  2\n"," 59 54  2 43 48 44  2 59 47 40 53  2 59 54  2 45 40 52 48 58 47 13  1  1\n"," 14 51 51 11  1 31 44 58 54 51 61 44 43  9  2 57 44 58 54 51 61 44 43  9\n","  1  1 19 48 57 58 59  2 16 48 59 48 65 44 53 11  1 19 48 57 58 59  7  2\n"," 64 54 60  2 50]\n","[53 54 62  2 16 40 48 60 58  2 26 40 57 42 48 60 58  2 48 58  2 42 47 48\n"," 44 45  2 44 53 44 52 64  2 59 54  2 59 47 44  2 55 44 54 55 51 44  9  1\n","  1 14 51 51 11  1 36 44  2 50 53 54 62  6 59  7  2 62 44  2 50 53 54 62\n","  6 59  9  1  1 19 48 57 58 59  2 16 48 59 48 65 44 53 11  1 25 44 59  2\n"," 60 58  2 50 48]\n","[51 51  2 47 48 52  7  2 40 53 43  2 62 44  6 51 51  2 47 40 61 44  2 42\n"," 54 57 53  2 40 59  2 54 60 57  2 54 62 53  2 55 57 48 42 44  9  1 22 58\n","  6 59  2 40  2 61 44 57 43 48 42 59 13  1  1 14 51 51 11  1 27 54  2 52\n"," 54 57 44  2 59 40 51 50 48 53 46  2 54 53  6 59 12  2 51 44 59  2 48 59\n","  2 41 44  2 43]\n","[54 53 44 11  2 40 62 40 64  7  2 40 62 40 64  3  1  1 32 44 42 54 53 43\n","  2 16 48 59 48 65 44 53 11  1 28 53 44  2 62 54 57 43  7  2 46 54 54 43\n","  2 42 48 59 48 65 44 53 58  9  1  1 19 48 57 58 59  2 16 48 59 48 65 44\n"," 53 11  1 36 44  2 40 57 44  2 40 42 42 54 60 53 59 44 43  2 55 54 54 57\n","  2 42 48 59 48]\n"]}]},{"cell_type":"code","source":["# define a function to split a sequence into a feature vector and a label\n","def split_input_target(sequence):\n","  input_text = sequence[:-1]\n","  target_text = sequence[1:]\n","  return input_text, target_text\n","\n","# example\n","split_input_target(list(\"Megasxlr\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"soe7AwwQ7FLe","executionInfo":{"status":"ok","timestamp":1661595053448,"user_tz":-60,"elapsed":384,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"94c44a8c-f08e-45a8-cc33-c4640fea59c0"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(['M', 'e', 'g', 'a', 's', 'x', 'l'], ['e', 'g', 'a', 's', 'x', 'l', 'r'])"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["# Apply the function into the batched sequence\n","dataset = sequences.map(split_input_target)\n","print(dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CMpmNnZ9748I","executionInfo":{"status":"ok","timestamp":1661595056618,"user_tz":-60,"elapsed":313,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"69d11347-a063-42b7-b744-5ad35d7be2c9"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["<MapDataset element_spec=(TensorSpec(shape=(100,), dtype=tf.int64, name=None), TensorSpec(shape=(100,), dtype=tf.int64, name=None))>\n"]}]},{"cell_type":"code","source":["for input_example, target_example in dataset.take(1):\n","  print(f\"input: {text_from_ids(input_example).numpy()}\")\n","  print(f\"label: {text_from_ids(target_example).numpy()}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"50YXB4v78srF","executionInfo":{"status":"ok","timestamp":1661595059613,"user_tz":-60,"elapsed":242,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"4b516c7f-0e88-495d-b082-a437af83494c"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["input: b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n","label: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"]}]},{"cell_type":"markdown","source":["wow, the feature and label are not so different."],"metadata":{"id":"dxiVm1ZI9EhL"}},{"cell_type":"markdown","source":["**Create training batches from the mapped dataset**"],"metadata":{"id":"kHBywQH2-W6-"}},{"cell_type":"code","source":["BATCH_SIZE = 64\n","BUFFER_SIZE = 10000\n","\n","dataset = (dataset\n","           .shuffle(BUFFER_SIZE)\n","           .batch(BATCH_SIZE, drop_remainder=True)\n","           .prefetch(tf.data.experimental.AUTOTUNE))\n","\n","dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZPlvqMx3-Wa2","executionInfo":{"status":"ok","timestamp":1661595067778,"user_tz":-60,"elapsed":675,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"c986af31-d67d-4628-fb14-09578a81a7ba"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"]},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","source":["Summary of text processing pipeline and generating feature and labels\n","1. Split the text into individual chars\n","2. Convert the individual chars into tokens\n","3. From the list of tokens generate a batch containing sequences of 101 tokens.\n","4. Split each batch into a feature vector and label\n","  - feature is the batch sequence excluding the last char (first 100 tokens)\n","  - label is the batch sequence excluding the first char (last 100 tokens)\n","5. Generate a new dataset containing batchs of 64 samples (feature vectors and labels)"],"metadata":{"id":"PVvfgQOw_kET"}},{"cell_type":"markdown","source":["## **Define the text generation model**"],"metadata":{"id":"hLunlV1UBj5P"}},{"cell_type":"code","source":["# define the model parameters\n","vocab_size = len(ids_from_chars.get_vocabulary())\n","print(vocab_size)\n","\n","embedding_dim = 256\n","rnn_units = 1024"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cYzker-iBngu","executionInfo":{"status":"ok","timestamp":1661595439989,"user_tz":-60,"elapsed":1072,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"4853c580-a623-456f-de7b-e7e3efe262f5"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["66\n"]}]},{"cell_type":"code","source":["# define the model\n","class MyModel(tf.keras.Model):\n","\n","  # define class constructor to initialise the layers with the parameters\n","  def __init__(self, vocab_size, embedding_dim, rnn_units):\n","    super().__init__(self)\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","    self.gru = tf.keras.layers.GRU(rnn_units, return_sequences=True,\n","                                   return_state=True)\n","    self.dense = tf.keras.layers.Dense(vocab_size)\n","  \n","\n","  # What are the requirement for defining sub class models derived from the Model class\n","  def call(self, inputs, states=None, return_state=False, training=False):\n","    x = inputs\n","\n","    # pass input to the embedding layer\n","    x = self.embedding(x, training=training)\n","\n","    # set initial state \n","    if states is None:\n","      states = self.gru.get_initial_state(x)\n","    \n","    # call the GRU layer with the embedding, initial state and training\n","    # what is training?\n","    x, states = self.gru(x, initial_state=states, training=training)\n","\n","    # Call the dense layer with the output of the GRU layer\n","    x = self.dense(x, training=training)\n","\n","    # dense output and state if specified.\n","    if return_state:\n","      return x, states\n","    else:\n","      return x\n","     "],"metadata":{"id":"wu5mB0FzB-Mg","executionInfo":{"status":"ok","timestamp":1661595442352,"user_tz":-60,"elapsed":230,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["# define an instance of the model\n","model = MyModel(vocab_size, embedding_dim, rnn_units)"],"metadata":{"id":"PoCTLoy0FfV2","executionInfo":{"status":"ok","timestamp":1661595462113,"user_tz":-60,"elapsed":436,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}}},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":["try the model on a single sample from the dataset"],"metadata":{"id":"UPyzw67KHnKE"}},{"cell_type":"code","source":["for batch_feature_vector, batch_label in dataset.take(1):\n","  example_batch_predictions = model(batch_feature_vector)\n","  print(batch_feature_vector.shape)\n","  print(example_batch_predictions.shape, \"(batch size, sequence_length, vocab_size)\\n\\n\")\n","  #print(example_batch_predictions)"],"metadata":{"id":"SMYx7X3PHfVn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["There are 64 samples in a batch.\n","\n","For each sample the model would predict a sequence with a length of 100. Recap that the label is a sequence of tokens and not an individual char. \n","Also recap that return sequence is set to True for the model so it would produce an output for each token in the sequence.\n","\n","Finally the dense layer has 66 neurons, so it would predict 66 diffrent values for each input.\n","\n","\n","Another explanation on the shape of the model output.   \n","As the it iterates through each token in the sequence of length 100, the model generates a prediction containing 66 values. This prediction is the probability distribution of the next probable char in the sequence.\n","\n","So we end up with 100 x 66 probability distribution for each token in the sequenece."],"metadata":{"id":"-5JMPDSw6C60"}},{"cell_type":"code","source":["for i in range(0, 100):\n","  #print(f\"probability distribution for next char: {example_batch_predictions[0][i]}\")\n","  print(f\"predicted class for the next char: {np.argmax(example_batch_predictions[0][i])}\\\n","   predicted char: {chars_from_ids(np.argmax(example_batch_predictions[0][i]))}\")"],"metadata":{"id":"XvPWXDfp4g3r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# I'm assuming that the model outputs a probabilty distribution for each input seeing as the number of units in the dense layer is equal to the vocab size\n","predicted_token = [np.argmax(distribution) for distribution in example_batch_predictions[0].numpy()]\n","predicted_chars = [chars_from_ids(token).numpy().decode('utf-8') for token in predicted_token]\n","\n","predicted_string = \"\".join(predicted_chars)\n","print(predicted_string)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tUU4pkbk1Fk7","executionInfo":{"status":"ok","timestamp":1661596301826,"user_tz":-60,"elapsed":211,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"8c1d7ba3-f3a4-4a4c-b668-51203661fe82"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":[" lulSSolhzhivD..;loloolj\n","hSl3;;X&.i,..Ussih  hSSo:l..llj;lj;Sol.cDo.ojSSo.rrPS;:XllMarDvD[UNK]Dmh-wrwlfN\n"]}]},{"cell_type":"markdown","source":["The next probable word would be determined from the output distribution produced by the dense layer."],"metadata":{"id":"mggq1npdJWBM"}},{"cell_type":"code","source":["# Is the sum of the assumed distribution equal to 1\n","sum_of_assumed_distribution = [np.sum(assumed_distribution) for assumed_distribution in example_batch_predictions[0].numpy()]\n","print(sum_of_assumed_distribution)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V6lM5yzH660C","executionInfo":{"status":"ok","timestamp":1661597451142,"user_tz":-60,"elapsed":207,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"7ac805e3-9694-4626-bf56-82ed47fdfa16"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["[-0.036192082, -0.100264624, 0.019551916, 0.10647095, 0.13143021, 0.036574095, 0.116993666, 0.15830699, 0.030974502, 0.025985744, -0.054978117, -0.11925046, 0.015699547, 0.10477143, -0.018283477, -0.042313967, -0.08930658, 0.0482501, 0.13610755, 0.04593085, 0.122342065, 0.1647057, 0.023054179, -0.009273745, -0.09761684, -0.10142118, 0.01766396, -0.066134945, -0.00088376366, -0.14412998, -0.15900391, -0.0009611242, -0.06818372, -0.12073738, -0.1522885, -0.15655327, -0.14487398, -0.2590501, -0.21714349, -0.17405833, 0.006345008, 0.020538159, -0.0562195, -0.06491099, -0.22549288, -0.1968332, -0.046858937, -0.063140176, -0.03473142, -0.07979047, 0.05666147, -0.039546542, -0.0500682, 0.06933912, -0.032980267, -0.04307981, -0.09222884, 0.04706017, 0.016584598, -0.053318217, 0.0377983, 0.121271715, 0.1634708, 0.016889958, -0.03086875, 0.06405425, -0.06029231, -0.060481455, -0.03330439, -0.033128005, 0.04792918, -0.061681267, 0.08374953, 0.019326497, 0.05886274, 0.0750042, -0.05857191, -0.026395936, -0.14921772, -0.068623625, -0.013929838, 0.06263299, -0.0051377937, -0.037225198, 0.04525768, 0.08138776, 0.1680422, 0.14338015, 0.16121195, 0.034811944, -0.06134776, -0.121970475, 0.047347955, 0.010997258, 0.06248468, 0.08320488, -0.040987138, 0.04560435, 0.088323355, 0.013837472]\n"]}]},{"cell_type":"markdown","source":["it looks as if the model is not predicting a probability distribution for each input, so how is the next char predicted from the model's output??"],"metadata":{"id":"NEzQ2EU97gTI"}},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_WCtAEcTHwFi","executionInfo":{"status":"ok","timestamp":1661595655310,"user_tz":-60,"elapsed":357,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"2186833f-1241-4af5-ba63-40f318a05cd3"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"my_model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       multiple                  16896     \n","                                                                 \n"," gru (GRU)                   multiple                  3938304   \n","                                                                 \n"," dense (Dense)               multiple                  67650     \n","                                                                 \n","=================================================================\n","Total params: 4,022,850\n","Trainable params: 4,022,850\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["# Take the first sample in the batch of 64\n","print(example_batch_predictions[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Be60bPq-JjJm","executionInfo":{"status":"ok","timestamp":1661595663324,"user_tz":-60,"elapsed":3,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"e4af738a-74f5-4f9f-9faa-8e9b01e76375"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[-1.76655850e-03 -2.36624526e-03  1.48365814e-02 ... -8.10714625e-03\n","   2.89362529e-03  1.59120071e-03]\n"," [-2.67120870e-03  7.79632665e-03  1.29185822e-02 ...  1.02399159e-02\n","  -8.53871275e-03 -9.98578034e-03]\n"," [ 4.10384359e-03 -1.14062941e-03  7.91937299e-03 ...  8.41901638e-05\n","   3.16182151e-04 -1.42250005e-02]\n"," ...\n"," [ 1.13328472e-02  8.69241729e-03  2.03810260e-03 ... -2.08790274e-03\n","  -5.54079190e-03 -3.73866968e-03]\n"," [ 9.16255545e-03  7.48849940e-03 -8.99519678e-03 ... -5.08212904e-03\n","  -1.10344915e-02  4.23197169e-04]\n"," [ 1.13810971e-02 -4.56483290e-03  8.98127630e-03 ...  4.93140658e-03\n","  -4.98686358e-03 -6.07391447e-03]], shape=(100, 66), dtype=float32)\n"]}]},{"cell_type":"code","source":["sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n","print(sampled_indices)"],"metadata":{"id":"-2zeVRgFKicE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Looking at the docs for [tf.random.categorical](https://www.tensorflow.org/api_docs/python/tf/random/categorical) it draws samples from a categorical distribution, so it seems to me that it just takes a sample from the *categorical distribution*."],"metadata":{"id":"ijJNGODs7zUo"}},{"cell_type":"code","source":["sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n","print(sampled_indices)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lqlwq1LpKzdL","executionInfo":{"status":"ok","timestamp":1661595685431,"user_tz":-60,"elapsed":351,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"05cab774-cef9-4e9a-fffb-f075ea55a574"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["[16 56 44 64 37 50  4  5  2 15  7 62 15 61 26 43 41 62 20 38  5 51  3 49\n"," 43 39 14 55 59  3 26 13 42 57 51  0  9  2 47 43 41 27 59 47 11 59 14 40\n"," 32 15 26 18 30 41 38 23 34 24 43  5 56 35 42 23 21  1 33 32 24 37 32 40\n"," 22 58 23 51  7  4 33 11 56 29 49 10 23 19 13 18 38 49 12 55 65 42 43  2\n"," 36 36 63 37]\n"]}]},{"cell_type":"code","source":["# display the input and model prediction\n","print(\"Input:\\n\", text_from_ids(batch_feature_vector[0]).numpy())\n","print(\"Expected label: \\n\", text_from_ids(batch_label[0]).numpy())\n","print(\"Predicted label: \\n\", text_from_ids(sampled_indices).numpy())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mlt04gztLcqt","executionInfo":{"status":"ok","timestamp":1661595697134,"user_tz":-60,"elapsed":525,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"58d8eab4-0128-4ec4-ec75-44379676cec4"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["Input:\n"," b\"nt; and every one did threat\\nTo-morrow's vengeance on the head of Richard.\\n\\nRATCLIFF:\\nMy lord!\\n\\nKING\"\n","Expected label: \n"," b\"t; and every one did threat\\nTo-morrow's vengeance on the head of Richard.\\n\\nRATCLIFF:\\nMy lord!\\n\\nKING \"\n","Predicted label: \n"," b'CqeyXk$& B,wBvMdbwGY&l!jdZApt!M?crl[UNK]. hdbNth:tAaSBMEQbYJUKd&qVcJH\\nTSKXSaIsJl,$T:qPj3JF?EYj;pzcd WWxX'\n"]}]},{"cell_type":"markdown","source":["I'm not sure why the tf.random.categorical is used here"],"metadata":{"id":"3_Qp44Xn9bXM"}},{"cell_type":"code","source":["# try out the random categorical\n","input = tf.math.log([[0.75, 0.75, 0.75]])\n","samples = tf.random.categorical(input, 100)\n","\n","print(input)\n","print(samples)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jOamwinZE-gE","executionInfo":{"status":"ok","timestamp":1661601156065,"user_tz":-60,"elapsed":223,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"f79fe0e8-cccc-44c3-8ea3-39433a1fe886"},"execution_count":67,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor([[-0.28768206 -0.28768206 -0.28768206]], shape=(1, 3), dtype=float32)\n","tf.Tensor(\n","[[1 2 0 1 0 0 2 2 2 2 0 1 1 0 1 1 0 0 1 0 0 1 1 0 0 1 2 2 1 2 0 1 0 1 2 0\n","  2 0 1 1 1 1 2 2 0 0 0 2 0 0 0 1 1 1 1 1 1 2 1 1 2 2 0 2 2 2 0 2 0 2 0 2\n","  0 2 1 0 2 1 1 2 2 1 0 2 0 2 2 1 0 1 0 1 2 2 2 2 2 2 2 2]], shape=(1, 100), dtype=int64)\n"]}]},{"cell_type":"markdown","source":["looking the output, the random.categorical function takes sampes from the categorical distribution and returns the class index. \n","\n","So above from the input with shape `[batch_size, num_class]` it would randomly take a sample from the input and return it's index."],"metadata":{"id":"RPRY_Oo2G1cs"}},{"cell_type":"code","source":["from collections import Counter\n","\n","samples_counts = Counter(samples[0].numpy())\n","print(f\"Number of occurance of 0 in samples : {samples_counts[0]}\")\n","print(f\"Number of occurance of 1 in samples: {samples_counts[1]}\")\n","print(f\"Number of occurance of 2 in samples: {samples_counts[2]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1I0pUVNOIEmS","executionInfo":{"status":"ok","timestamp":1661601159190,"user_tz":-60,"elapsed":195,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"263e2807-dfa8-4974-f944-21b96c89e1f0"},"execution_count":69,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of occurance of 0 in samples : 31\n","Number of occurance of 1 in samples: 32\n","Number of occurance of 2 in samples: 37\n"]}]},{"cell_type":"markdown","source":["it also looks like the number of samples taken for each class is determined by the probability of that class in the input logits.\n","\n","Recap that logits: *\"unnormalized log-probabilities for all classes\"*   \n","\n","Just to summarise.\n","- Using random categorical, it would select n samples from a set of unnormalized log-probabilities. It doesn't select samples with the highest probabilities but rather the probability indicates the likelihood that the sample is selected."],"metadata":{"id":"-Abnz09KJfS5"}},{"cell_type":"code","source":["print(example_batch_predictions[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f5y-h2OOFJ7q","executionInfo":{"status":"ok","timestamp":1661596427048,"user_tz":-60,"elapsed":223,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"9d803c29-df08-4ac3-db6f-bf12546ae310"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[-1.76655850e-03 -2.36624526e-03  1.48365814e-02 ... -8.10714625e-03\n","   2.89362529e-03  1.59120071e-03]\n"," [-2.67120870e-03  7.79632665e-03  1.29185822e-02 ...  1.02399159e-02\n","  -8.53871275e-03 -9.98578034e-03]\n"," [ 4.10384359e-03 -1.14062941e-03  7.91937299e-03 ...  8.41901638e-05\n","   3.16182151e-04 -1.42250005e-02]\n"," ...\n"," [ 1.13328472e-02  8.69241729e-03  2.03810260e-03 ... -2.08790274e-03\n","  -5.54079190e-03 -3.73866968e-03]\n"," [ 9.16255545e-03  7.48849940e-03 -8.99519678e-03 ... -5.08212904e-03\n","  -1.10344915e-02  4.23197169e-04]\n"," [ 1.13810971e-02 -4.56483290e-03  8.98127630e-03 ...  4.93140658e-03\n","  -4.98686358e-03 -6.07391447e-03]], shape=(100, 66), dtype=float32)\n"]}]},{"cell_type":"code","source":["print(batch_label[0])"],"metadata":{"id":"UjWuvfpyFPVZ","executionInfo":{"status":"ok","timestamp":1661596430498,"user_tz":-60,"elapsed":234,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"5f1c6a5f-4522-409f-efad-9b1c1f48766e","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[59 12  2 40 53 43  2 44 61 44 57 64  2 54 53 44  2 43 48 43  2 59 47 57\n"," 44 40 59  1 33 54  8 52 54 57 57 54 62  6 58  2 61 44 53 46 44 40 53 42\n"," 44  2 54 53  2 59 47 44  2 47 44 40 43  2 54 45  2 31 48 42 47 40 57 43\n","  9  1  1 31 14 33 16 25 22 19 19 11  1 26 64  2 51 54 57 43  3  1  1 24\n"," 22 27 20  2], shape=(100,), dtype=int64)\n"]}]},{"cell_type":"code","source":["# define a loss function for the model\n","loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n","\n","# get the loss for the first batch in the dataset\n","example_batch_mean_loss = loss(batch_label, example_batch_predictions)\n","print(f\"Prediction shape: {example_batch_predictions.shape}\")\n","print(f\"Mean loss: {example_batch_mean_loss}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AoezGOm6MqA3","executionInfo":{"status":"ok","timestamp":1661596432709,"user_tz":-60,"elapsed":201,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"cb81edba-6b4c-4178-a6c9-79f29f00e068"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["Prediction shape: (64, 100, 66)\n","Mean loss: 4.187921524047852\n"]}]},{"cell_type":"code","source":["tf.exp(example_batch_mean_loss).numpy()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A8fafNGJOhqB","executionInfo":{"status":"ok","timestamp":1661596460650,"user_tz":-60,"elapsed":224,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"787b07a1-c533-4394-9727-0f76249d165c"},"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["65.88571"]},"metadata":{},"execution_count":47}]},{"cell_type":"markdown","source":["I'm not too sure about this, but \n","- *The exponential of the mean loss should be approximately equal to the vocabulary size*. As the output logits from the dense layer should have similar magnitudes."],"metadata":{"id":"SRRYc7RlPGho"}},{"cell_type":"markdown","source":["## **Train the model**"],"metadata":{"id":"CE0j4P1sMim4"}},{"cell_type":"code","source":["# define the loss and optimizer for the model\n","model.compile(optimizer='adam', loss=loss)\n"],"metadata":{"id":"YR4tMxeqO4oH","executionInfo":{"status":"ok","timestamp":1661597279336,"user_tz":-60,"elapsed":289,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}}},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":["**Define model callbacks**\n","\n","ModelCheckpoint\n","- Saves model/ weight at a defined frequency. (So it saves the model or it's weight at different point during training)"],"metadata":{"id":"Fki_zjlDPoql"}},{"cell_type":"code","source":["# define a directory to store checkoints of the model during training\n","checkpoint_dir = \"./training_checkpoints\"\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n","\n","# define the model checkpoint\n","# Saves the model weight at the end of each epoch to the training_checkpoints dir\n","checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n","                                                         save_weights_only=True)\n"],"metadata":{"id":"aCCPru2nPmMv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the training epochs \n","EPOCHS = 20"],"metadata":{"id":"LNu4wQI5R5ZV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"],"metadata":{"id":"ykwqoDcwR_gG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Generating Text**\n","\n","In generating text, we would provide a seed character and then run inference to predict the next probable characther, running this multiple times would allow us to generate larger pieces of text."],"metadata":{"id":"f2Vk2rqHT-6C"}},{"cell_type":"code","source":["class OneStep(tf.keras.Model):\n","  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n","    super().__init__()\n","    self.temperature = temperature\n","    self.model = model\n","    self.chars_from_ids = chars_from_ids\n","    self.ids_from_chars = ids_from_chars\n","  \n","    # Create a mask to prevent \"[UNK]\" from being generated.\n","    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n","    sparse_mask = tf.SparseTensor(\n","        # Put a -inf at each bad index\n","        values=[-float('inf')]*len(skip_ids),\n","        indices=skip_ids,\n","        # Match the shape of the vocabulary\n","        dense_shape=[len(ids_from_chars.get_vocabulary())])\n","    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n","  \n","\n","  @tf.function\n","  def generate_one_step(self, inputs, states=None):\n","    # Convert strings to token IDs\n","    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n","    input_ids = self.ids_from_chars(input_chars).to_tensor()\n","\n","    # Run the model.\n","    # predicted_logits.shape is [batch, char, next_char_logits]\n","    predicted_logits, states = self.model(inputs=input_ids, states=states, return_state=True)\n","\n","    # Only use the last prediction.\n","    predicted_logits = predicted_logits[:, -1, :]\n","    predicted_logits = predicted_logits/self.temperature\n","    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n","    predicted_logits = predicted_logits + self.prediction_mask\n","\n","    # Sample the output logits to generate token IDs.\n","    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n","    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n","\n","    # Convert from token ids to characters\n","    predicted_chars = self.chars_from_ids(predicted_ids)\n","    \n","    # Return the characters and model state.\n","    return predicted_chars, states"],"metadata":{"id":"fIRADR6nT9vH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"],"metadata":{"id":"ZfVa1MEVacKe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["start = time.time()\n","states = None\n","next_char = tf.constant(['Post MALONE:'])\n","result = [next_char]\n","\n","for n in range(1000):\n","  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n","  result.append(next_char)\n","\n","\n","result = tf.strings.join(result)\n","end = time.time()\n","print(result[0].numpy().decode('utf-8'), '\\n\\n', '_'*80)\n","print('\\nRun time:', end - start)"],"metadata":{"id":"BPaWE16Yajhc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Save the text generator**"],"metadata":{"id":"qGEcOjn-EQeD"}},{"cell_type":"code","source":["# Save the model\n","tf.saved_model.save(one_step_model, 'one_step')\n"],"metadata":{"id":"6VItcykdEU4B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load the model\n","one_step_model_reloaded = tf.saved_model.load('one_step')"],"metadata":{"id":"WQDJpvC8Ech5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Advanced: Customized Training**"],"metadata":{"id":"ye52cOY4FNVr"}},{"cell_type":"code","source":["class CustomTraining(MyModel):\n","  @tf.function\n","  def train_step(self, inputs):\n","    inputs, labels = inputs\n","    with tf.GradientTape() as tape:\n","      predictions = self(inputs, training=True)\n","      loss = self.loss(labels, predictions)\n","    grads = tape.gradient(loss, model.trainable_variables)\n","    self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n","\n","    return {'loss': loss}\n","  \n"],"metadata":{"id":"i1g79dV3EwkC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = CustomTraining(\n","    vocab_size=len(ids_from_chars.get_vocabulary()),\n","    embedding_dim=embedding_dim,\n","    rnn_units=rnn_units)"],"metadata":{"id":"K_sU4rYeGJPB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.compile(optimizer = tf.keras.optimizers.Adam(),\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"],"metadata":{"id":"0C2HLKwVGZ-Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.fit(dataset, epochs=1)"],"metadata":{"id":"NxNwo-9WGmip"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["EPOCHS = 10\n","\n","mean = tf.metrics.Mean()\n","\n","for epoch in range(EPOCHS):\n","  start = time.time()\n","\n","  mean.reset_states()\n","  for (batch_n, (inp, target)) in enumerate(dataset):\n","    logs = model.train_staep([inp, target])\n","    mean.update_state(logs['loss'])\n","\n","    if batch_n % 50 == 0:\n","      template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n","      print(template)\n","    \n","    # saving (checkpoint) the model every 5 epochs\n","    if (epoch + 1) % 5 == 0:\n","      model.save-weights(checkpoint_prefix.format(epoch=epoch))\n","    \n","\n","    print()\n","    print(f\"Epoch {epoch+1}, loss: {mean.result().numpy():.4f}\")\n","    print(f\"Time taken for 1 epoch {time.time() - start:.2f} sec\")\n","    print(\"_\"*80)"],"metadata":{"id":"lUKqcULPGuM9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Define and train a model**"],"metadata":{"id":"EXwg0KpAyaB0"}},{"cell_type":"code","source":[],"metadata":{"id":"nKUEUdnTyahc"},"execution_count":null,"outputs":[]}]}