{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Text Generation using RNN.ipynb","provenance":[],"collapsed_sections":["CE0j4P1sMim4","f2Vk2rqHT-6C"],"toc_visible":true,"authorship_tag":"ABX9TyPFYQIRIHB0HBXn41Spi2bC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# **Text generation with an RNN**\n","\n","following [tensorflow tutorial](https://www.tensorflow.org/text/tutorials/text_generation).\n","\n","This tutorial looks at training an RNN model to predict the next character in a sequence. The model is trained on text written by shakespeare.\n","\n","Something to keep in mind. Text generation process demonstrated in Udacity.\n","- Tokenization, followed by converting the texts to sequences.\n","\n","- Each sequence would have a set length. From each sequences, we use the last token as the label and the remaining sequences as the feature vector.\n","\n","- Convert the labels into one-hot vectors, with it's length being the vocabulary size\n","\n","- Train a classification model on the sequences and one-hot labels.\n","\n","- We would then generate text by providing a seed word followed by multiple inference.\n"],"metadata":{"id":"fhLXjY4bLaue"}},{"cell_type":"markdown","source":["##**Import dependencies**"],"metadata":{"id":"yiHsY9gYoTKL"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"rFLuAWMjJUjE","executionInfo":{"status":"ok","timestamp":1660068051778,"user_tz":-60,"elapsed":2480,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4615ab80-f3f3-478c-f316-237c94ebc845"},"outputs":[{"output_type":"stream","name":"stdout","text":["2.8.2\n"]}],"source":["import tensorflow as tf\n","import numpy as np\n","import os\n","import time\n","\n","print(tf.__version__)"]},{"cell_type":"markdown","source":["## **Get the dataset**"],"metadata":{"id":"Zm98stqCodZr"}},{"cell_type":"code","source":["path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-cLRkrGcohJ4","executionInfo":{"status":"ok","timestamp":1660068056978,"user_tz":-60,"elapsed":280,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"5fe6194d-dae7-4182-b354-befb6e3db496"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n","1122304/1115394 [==============================] - 0s 0us/step\n","1130496/1115394 [==============================] - 0s 0us/step\n"]}]},{"cell_type":"code","source":["# decode the text\n","text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n","print(text)"],"metadata":{"id":"iCBWlJCppUpm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"Number of characters in the text: {len(text)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hZgnWxdKpsi3","executionInfo":{"status":"ok","timestamp":1660068064345,"user_tz":-60,"elapsed":10,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"a0f2ff95-b4fa-45e6-822e-958a5ff77af3"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of characters in the text: 1115394\n"]}]},{"cell_type":"code","source":["# find the number of unique characters in the file\n","vocab = sorted(set(text))\n","print(vocab)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fHTHgiL5p0bX","executionInfo":{"status":"ok","timestamp":1660068072452,"user_tz":-60,"elapsed":9,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"14552866-5a9e-424f-d67a-4a3b05380566"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"]}]},{"cell_type":"code","source":["print(f\"Number of unique charachter in the text is {len(vocab)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AwLD6BSUq1pf","executionInfo":{"status":"ok","timestamp":1660068078159,"user_tz":-60,"elapsed":215,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"2dae88e0-06d3-432b-aee5-86ff23832b81"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of unique charachter in the text is 65\n"]}]},{"cell_type":"markdown","source":["The approach used here was to download the data as a file, read and then decode it into a list. This is miles better than downloading it  using tensorflow dataset and then loading it.\n","\n","An intresting note there, the vocab size was determined by the number of unique characthers in the dataset. This aligns with the task the model would be trained to do which is to predict the next probable characther given an initial characther.\n"],"metadata":{"id":"cwrpIFmGxMNv"}},{"cell_type":"markdown","source":["## **Process the text**"],"metadata":{"id":"CC4g2LEArRQG"}},{"cell_type":"code","source":["# encode sample string into utf-8 format\n","example_texts = [\"megasxlr\", \"theweekend\"]\n","\n","example_text_chars = tf.strings.unicode_split(example_texts, input_encoding=\"UTF-8\")\n","print(example_text_chars)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sOUDLS05rL8E","executionInfo":{"status":"ok","timestamp":1660068267177,"user_tz":-60,"elapsed":606,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"dd9ad992-278e-4966-fdac-95114d95b228"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["<tf.RaggedTensor [[b'm', b'e', b'g', b'a', b's', b'x', b'l', b'r'],\n"," [b't', b'h', b'e', b'w', b'e', b'e', b'k', b'e', b'n', b'd']]>\n"]}]},{"cell_type":"code","source":["for index, char_ in enumerate(list(vocab)):\n","  print(f\"index:{index}, character:{char_}\")"],"metadata":{"id":"6P8FD7DZu1y6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Define a text encoder**"],"metadata":{"id":"0ZScqz5b6ig4"}},{"cell_type":"code","source":["# create an encoder to convert the string into token\n","ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab),\n","                                              mask_token=None)\n"],"metadata":{"id":"6WiHkOyduR_d","executionInfo":{"status":"ok","timestamp":1660068293204,"user_tz":-60,"elapsed":457,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["ids = ids_from_chars(example_text_chars)\n","print(ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_30JjnATunoN","executionInfo":{"status":"ok","timestamp":1660068297781,"user_tz":-60,"elapsed":212,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"e7ccc874-99af-45e7-c700-016df9a85367"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["<tf.RaggedTensor [[52, 44, 46, 40, 58, 63, 51, 57],\n"," [59, 47, 44, 62, 44, 44, 50, 44, 53, 43]]>\n"]}]},{"cell_type":"markdown","source":["it seems like it's encoded the text using the index from the initial vocab (with a slight offset).\n","\n","It looks like [**`tf.keras.layers.StringLookUp`**](https://www.tensorflow.org/api_docs/python/tf/keras/layers/StringLookup) is an alternative to the Tokenizer function which is deprecated in v2.9. The StringLookup is also able to create one-hot vectors to use as tokens for each characthers.\n","\n","Another alternative for tokenization is the [TextVectorization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization)"],"metadata":{"id":"y1G0bLX0valy"}},{"cell_type":"markdown","source":["**Define a text decoder**"],"metadata":{"id":"jrHsi_oS6bm0"}},{"cell_type":"code","source":["# create a text decoder\n","chars_from_ids = tf.keras.layers.StringLookup(vocabulary=ids_from_chars.get_vocabulary(),\n","                                              invert=True,\n","                                              mask_token=None)\n"],"metadata":{"id":"57bIaqxouwDx","executionInfo":{"status":"ok","timestamp":1660068821091,"user_tz":-60,"elapsed":337,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["decoded_chars = chars_from_ids(ids)\n","print(decoded_chars)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TDYqSrLOwGXt","executionInfo":{"status":"ok","timestamp":1660068823684,"user_tz":-60,"elapsed":20,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"499f8ca9-d49c-4369-8560-0bb44f1b9603"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["<tf.RaggedTensor [[b'm', b'e', b'g', b'a', b's', b'x', b'l', b'r'],\n"," [b't', b'h', b'e', b'w', b'e', b'e', b'k', b'e', b'n', b'd']]>\n"]}]},{"cell_type":"code","source":["# the decoded chars are returns as list of char. We can join the individual\n","# chars back into a string\n","tf.strings.reduce_join(decoded_chars, axis=-1).numpy()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sScbJxSDwf3G","executionInfo":{"status":"ok","timestamp":1660068830207,"user_tz":-60,"elapsed":837,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"bbf86abc-ab12-464f-d08f-ff2462e65613"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([b'megasxlr', b'theweekend'], dtype=object)"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["print(decoded_chars)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Nfo5imlxFIG","executionInfo":{"status":"ok","timestamp":1660068861678,"user_tz":-60,"elapsed":666,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"a30c0429-c752-44a1-e47e-dd7f85d4d3c1"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["<tf.RaggedTensor [[b'm', b'e', b'g', b'a', b's', b'x', b'l', b'r'],\n"," [b't', b'h', b'e', b'w', b'e', b'e', b'k', b'e', b'n', b'd']]>\n"]}]},{"cell_type":"code","source":["def text_from_ids(ids):\n","  \"\"\"Function defined to convert string tokens back into an array of strings.\"\"\"\n","  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"],"metadata":{"id":"GOzuCYOC06z0","executionInfo":{"status":"ok","timestamp":1660068925040,"user_tz":-60,"elapsed":212,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["The trained model would be able to predict the next probable character given an initial characther or sequence of characther.\n","\n","Towards this\n","- our training dataset needs to contain text where the input contain parts of the text and label contains the remaining parts of it.\n","- Example: Input: Megas, Label: Megasx\n"],"metadata":{"id":"ZrgT2gB9yRhm"}},{"cell_type":"code","source":["# convert the individual text in the dialog into chars\n","text_split_into_individual_chars = tf.strings.unicode_split(text, 'UTF-8')\n","print(text_split_into_individual_chars)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X9qwEvGD0wpy","executionInfo":{"status":"ok","timestamp":1660069041250,"user_tz":-60,"elapsed":1499,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"dff12bea-ce1e-439b-a3b0-22fd8f232bba"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor([b'F' b'i' b'r' ... b'g' b'.' b'\\n'], shape=(1115394,), dtype=string)\n"]}]},{"cell_type":"code","source":["# convert each characthers into tokens\n","text_ids = ids_from_chars(text_split_into_individual_chars)\n","print(text_ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QBE2QR0n2bCJ","executionInfo":{"status":"ok","timestamp":1660069041749,"user_tz":-60,"elapsed":11,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"2410485a-673e-41e9-8b3a-3f969265ddd8"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor([19 48 57 ... 46  9  1], shape=(1115394,), dtype=int64)\n"]}]},{"cell_type":"code","source":["# generate a tensorsliceDataset from the tensorslice\n","ids_dataset = tf.data.Dataset.from_tensor_slices(text_ids)\n","print(ids_dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IOyGyPYm2uXQ","executionInfo":{"status":"ok","timestamp":1660069043583,"user_tz":-60,"elapsed":11,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"0f39d54d-2bb0-43cc-9f42-b2ba22ad9aba"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["<TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n"]}]},{"cell_type":"code","source":["for ids in ids_dataset.take(10):\n","  print(chars_from_ids(ids).numpy().decode('utf-8'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9WhOEPJO3PKS","executionInfo":{"status":"ok","timestamp":1660069044308,"user_tz":-60,"elapsed":8,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"50243cba-aeb8-454e-e5ff-348b3c0d49f2"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["F\n","i\n","r\n","s\n","t\n"," \n","C\n","i\n","t\n","i\n"]}]},{"cell_type":"code","source":["# define the max_length of the sequences\n","seq_length = 100\n"],"metadata":{"id":"SbOSc8Zf3e5k","executionInfo":{"status":"ok","timestamp":1660069046746,"user_tz":-60,"elapsed":2,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["# from the dataset generate a batch with a length of 101\n","sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n","\n","# display a single batch\n","for seq in sequences.take(5):\n","  print(text_from_ids(seq).numpy())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rIdQDdyS3oev","executionInfo":{"status":"ok","timestamp":1660069072101,"user_tz":-60,"elapsed":514,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"eff88ee1-4ef2-4c9c-e501-0ba6b5b37036"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n","b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n","b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n","b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n","b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"]}]},{"cell_type":"code","source":["for seq in sequences.take(5):\n","  print(f\"{seq}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rSCJ1kPV1X33","executionInfo":{"status":"ok","timestamp":1660069213221,"user_tz":-60,"elapsed":216,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"d8e7bd23-ffda-47c3-fed4-ca4641fdf19d"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["[19 48 57 58 59  2 16 48 59 48 65 44 53 11  1 15 44 45 54 57 44  2 62 44\n","  2 55 57 54 42 44 44 43  2 40 53 64  2 45 60 57 59 47 44 57  7  2 47 44\n"," 40 57  2 52 44  2 58 55 44 40 50  9  1  1 14 51 51 11  1 32 55 44 40 50\n","  7  2 58 55 44 40 50  9  1  1 19 48 57 58 59  2 16 48 59 48 65 44 53 11\n","  1 38 54 60  2]\n","[40 57 44  2 40 51 51  2 57 44 58 54 51 61 44 43  2 57 40 59 47 44 57  2\n"," 59 54  2 43 48 44  2 59 47 40 53  2 59 54  2 45 40 52 48 58 47 13  1  1\n"," 14 51 51 11  1 31 44 58 54 51 61 44 43  9  2 57 44 58 54 51 61 44 43  9\n","  1  1 19 48 57 58 59  2 16 48 59 48 65 44 53 11  1 19 48 57 58 59  7  2\n"," 64 54 60  2 50]\n","[53 54 62  2 16 40 48 60 58  2 26 40 57 42 48 60 58  2 48 58  2 42 47 48\n"," 44 45  2 44 53 44 52 64  2 59 54  2 59 47 44  2 55 44 54 55 51 44  9  1\n","  1 14 51 51 11  1 36 44  2 50 53 54 62  6 59  7  2 62 44  2 50 53 54 62\n","  6 59  9  1  1 19 48 57 58 59  2 16 48 59 48 65 44 53 11  1 25 44 59  2\n"," 60 58  2 50 48]\n","[51 51  2 47 48 52  7  2 40 53 43  2 62 44  6 51 51  2 47 40 61 44  2 42\n"," 54 57 53  2 40 59  2 54 60 57  2 54 62 53  2 55 57 48 42 44  9  1 22 58\n","  6 59  2 40  2 61 44 57 43 48 42 59 13  1  1 14 51 51 11  1 27 54  2 52\n"," 54 57 44  2 59 40 51 50 48 53 46  2 54 53  6 59 12  2 51 44 59  2 48 59\n","  2 41 44  2 43]\n","[54 53 44 11  2 40 62 40 64  7  2 40 62 40 64  3  1  1 32 44 42 54 53 43\n","  2 16 48 59 48 65 44 53 11  1 28 53 44  2 62 54 57 43  7  2 46 54 54 43\n","  2 42 48 59 48 65 44 53 58  9  1  1 19 48 57 58 59  2 16 48 59 48 65 44\n"," 53 11  1 36 44  2 40 57 44  2 40 42 42 54 60 53 59 44 43  2 55 54 54 57\n","  2 42 48 59 48]\n"]}]},{"cell_type":"code","source":["# define a function to split a sequence into a feature vector and a label\n","def split_input_target(sequence):\n","  input_text = sequence[:-1]\n","  target_text = sequence[1:]\n","  return input_text, target_text\n","\n","# example\n","split_input_target(list(\"Megasxlr\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"soe7AwwQ7FLe","executionInfo":{"status":"ok","timestamp":1660069097522,"user_tz":-60,"elapsed":509,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"b14be587-d926-4933-de76-8669c1e18962"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(['M', 'e', 'g', 'a', 's', 'x', 'l'], ['e', 'g', 'a', 's', 'x', 'l', 'r'])"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["# Apply the function into the batched sequence\n","dataset = sequences.map(split_input_target)\n","print(dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CMpmNnZ9748I","executionInfo":{"status":"ok","timestamp":1660069104829,"user_tz":-60,"elapsed":557,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"98217f35-8e58-4d27-dfd2-0372f1ce9d99"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["<MapDataset element_spec=(TensorSpec(shape=(100,), dtype=tf.int64, name=None), TensorSpec(shape=(100,), dtype=tf.int64, name=None))>\n"]}]},{"cell_type":"code","source":["for input_example, target_example in dataset.take(1):\n","  print(f\"input: {text_from_ids(input_example).numpy()}\")\n","  print(f\"label: {text_from_ids(target_example).numpy()}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"50YXB4v78srF","executionInfo":{"status":"ok","timestamp":1660069174867,"user_tz":-60,"elapsed":589,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"26394534-6f24-4975-b385-2a8e55c83c9b"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["input: b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n","label: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"]}]},{"cell_type":"markdown","source":["wow, the feature and label are not so different."],"metadata":{"id":"dxiVm1ZI9EhL"}},{"cell_type":"markdown","source":["**Create training batches from the mapped dataset**"],"metadata":{"id":"kHBywQH2-W6-"}},{"cell_type":"code","source":["BATCH_SIZE = 64\n","BUFFER_SIZE = 10000\n","\n","dataset = (dataset\n","           .shuffle(BUFFER_SIZE)\n","           .batch(BATCH_SIZE, drop_remainder=True)\n","           .prefetch(tf.data.experimental.AUTOTUNE))\n","\n","dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZPlvqMx3-Wa2","executionInfo":{"status":"ok","timestamp":1660069267493,"user_tz":-60,"elapsed":466,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"34eedf3e-4888-40f1-e5ff-32008810c147"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"]},"metadata":{},"execution_count":27}]},{"cell_type":"markdown","source":["Summary of text processing pipeline and generating feature and labels\n","1. Split the text into individual chars\n","2. Convert the individual chars into tokens\n","3. From the list of tokens generate a batch containing sequences of 101 tokens.\n","4. Split each batch into a feature vector and label\n","  - feature is the batch sequence excluding the last char (first 100 tokens)\n","  - label is the batch sequence excluding the first char (last 100 tokens)\n","5. Generate a new dataset containing batchs of 64 samples (feature vectors and labels)"],"metadata":{"id":"PVvfgQOw_kET"}},{"cell_type":"markdown","source":["## **Define the text generation model**"],"metadata":{"id":"hLunlV1UBj5P"}},{"cell_type":"code","source":["# define the model parameters\n","vocab_size = len(ids_from_chars.get_vocabulary())\n","print(vocab_size)\n","\n","embedding_dim = 256\n","rnn_units = 1024"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cYzker-iBngu","executionInfo":{"status":"ok","timestamp":1660069388650,"user_tz":-60,"elapsed":276,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"5a0d46c0-c6ee-4785-9955-673abc1f2fd5"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["66\n"]}]},{"cell_type":"code","source":["# define the model\n","class MyModel(tf.keras.Model):\n","\n","  # define class constructor to initialise the layers with the parameters\n","  def __init__(self, vocab_size, embedding_dim, rnn_units):\n","    super().__init__(self)\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","    self.gru = tf.keras.layers.GRU(rnn_units, return_sequences=True,\n","                                   return_state=True)\n","    self.dense = tf.keras.layers.Dense(vocab_size)\n","  \n","\n","  # What are the requirement for defining sub class models derived from the Model class\n","  def call(self, inputs, states=None, return_state=False, training=False):\n","    x = inputs\n","\n","    # pass input to the embedding layer\n","    x = self.embedding(x, training=training)\n","\n","    # set initial state \n","    if states is None:\n","      states = self.gru.get_initial_state(x)\n","    \n","    # call the GRU layer with the embedding, initial state and training\n","    # what is training?\n","    x, states = self.gru(x, initial_state=states, training=training)\n","\n","    # Call the dense layer with the output of the GRU layer\n","    x = self.dense(x, training=training)\n","\n","    # dense output and state if specified.\n","    if return_state:\n","      return x, states\n","    else:\n","      return x\n","     "],"metadata":{"id":"wu5mB0FzB-Mg","executionInfo":{"status":"ok","timestamp":1660069647022,"user_tz":-60,"elapsed":220,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["# define an instance of the model\n","model = MyModel(vocab_size, embedding_dim, rnn_units)"],"metadata":{"id":"PoCTLoy0FfV2","executionInfo":{"status":"ok","timestamp":1660069650610,"user_tz":-60,"elapsed":202,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}}},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":["try the model on a single sample from the dataset"],"metadata":{"id":"UPyzw67KHnKE"}},{"cell_type":"code","source":["for batch_feature_vector, batch_label in dataset.take(1):\n","  example_batch_predictions = model(batch_feature_vector)\n","  print(example_batch_predictions.shape, \"(batch size, sequence_length, vocab_size)\\n\\n\")\n","  print(example_batch_predictions)"],"metadata":{"id":"SMYx7X3PHfVn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["There are 64 samples in a batch.\n","\n","For each sample the model would predict a sequence with a length of 100. Recap that the label is a sequence of tokens and not an individual char. \n","Also recap that return sequence is set to True for the model so it would produce an output for each token in the sequence.\n","\n","Finally the dense layer has 66 neurons, so it would predict 66 diffrent values for each input.\n","\n","\n","Another explanation on the shape of the model output.   \n","As the it iterates through each token in the sequence of length 100, the model generates a prediction containing 66 values. This prediction is the probability distribution of the next probable char in the sequence.\n","\n","So we end up with 100 x 66 probability distribution for each token in the sequenece."],"metadata":{"id":"-5JMPDSw6C60"}},{"cell_type":"code","source":["for i in range(0, 100):\n","  #print(f\"probability distribution for next char: {example_batch_predictions[0][i]}\")\n","  print(f\"predicted class for the next char: {np.argmax(example_batch_predictions[0][i])}\\\n","   predicted char: {chars_from_ids(np.argmax(example_batch_predictions[0][i]))}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XvPWXDfp4g3r","executionInfo":{"status":"ok","timestamp":1660071068091,"user_tz":-60,"elapsed":955,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"3774d02e-de60-4a66-eb9e-f118e01bf015"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["predicted class for the next char: 31   predicted char: b'R'\n","predicted class for the next char: 58   predicted char: b's'\n","predicted class for the next char: 5   predicted char: b'&'\n","predicted class for the next char: 3   predicted char: b'!'\n","predicted class for the next char: 31   predicted char: b'R'\n","predicted class for the next char: 15   predicted char: b'B'\n","predicted class for the next char: 24   predicted char: b'K'\n","predicted class for the next char: 7   predicted char: b','\n","predicted class for the next char: 52   predicted char: b'm'\n","predicted class for the next char: 52   predicted char: b'm'\n","predicted class for the next char: 7   predicted char: b','\n","predicted class for the next char: 7   predicted char: b','\n","predicted class for the next char: 7   predicted char: b','\n","predicted class for the next char: 24   predicted char: b'K'\n","predicted class for the next char: 24   predicted char: b'K'\n","predicted class for the next char: 7   predicted char: b','\n","predicted class for the next char: 24   predicted char: b'K'\n","predicted class for the next char: 52   predicted char: b'm'\n","predicted class for the next char: 9   predicted char: b'.'\n","predicted class for the next char: 11   predicted char: b':'\n","predicted class for the next char: 11   predicted char: b':'\n","predicted class for the next char: 31   predicted char: b'R'\n","predicted class for the next char: 21   predicted char: b'H'\n","predicted class for the next char: 38   predicted char: b'Y'\n","predicted class for the next char: 29   predicted char: b'P'\n","predicted class for the next char: 15   predicted char: b'B'\n","predicted class for the next char: 7   predicted char: b','\n","predicted class for the next char: 4   predicted char: b'$'\n","predicted class for the next char: 11   predicted char: b':'\n","predicted class for the next char: 5   predicted char: b'&'\n","predicted class for the next char: 3   predicted char: b'!'\n","predicted class for the next char: 13   predicted char: b'?'\n","predicted class for the next char: 36   predicted char: b'W'\n","predicted class for the next char: 7   predicted char: b','\n","predicted class for the next char: 54   predicted char: b'o'\n","predicted class for the next char: 18   predicted char: b'E'\n","predicted class for the next char: 51   predicted char: b'l'\n","predicted class for the next char: 52   predicted char: b'm'\n","predicted class for the next char: 29   predicted char: b'P'\n","predicted class for the next char: 29   predicted char: b'P'\n","predicted class for the next char: 59   predicted char: b't'\n","predicted class for the next char: 7   predicted char: b','\n","predicted class for the next char: 7   predicted char: b','\n","predicted class for the next char: 59   predicted char: b't'\n","predicted class for the next char: 58   predicted char: b's'\n","predicted class for the next char: 58   predicted char: b's'\n","predicted class for the next char: 3   predicted char: b'!'\n","predicted class for the next char: 5   predicted char: b'&'\n","predicted class for the next char: 9   predicted char: b'.'\n","predicted class for the next char: 20   predicted char: b'G'\n","predicted class for the next char: 45   predicted char: b'f'\n","predicted class for the next char: 4   predicted char: b'$'\n","predicted class for the next char: 12   predicted char: b';'\n","predicted class for the next char: 36   predicted char: b'W'\n","predicted class for the next char: 36   predicted char: b'W'\n","predicted class for the next char: 19   predicted char: b'F'\n","predicted class for the next char: 44   predicted char: b'e'\n","predicted class for the next char: 22   predicted char: b'I'\n","predicted class for the next char: 20   predicted char: b'G'\n","predicted class for the next char: 25   predicted char: b'L'\n","predicted class for the next char: 1   predicted char: b'\\n'\n","predicted class for the next char: 56   predicted char: b'q'\n","predicted class for the next char: 3   predicted char: b'!'\n","predicted class for the next char: 28   predicted char: b'O'\n","predicted class for the next char: 56   predicted char: b'q'\n","predicted class for the next char: 25   predicted char: b'L'\n","predicted class for the next char: 25   predicted char: b'L'\n","predicted class for the next char: 25   predicted char: b'L'\n","predicted class for the next char: 24   predicted char: b'K'\n","predicted class for the next char: 24   predicted char: b'K'\n","predicted class for the next char: 42   predicted char: b'c'\n","predicted class for the next char: 15   predicted char: b'B'\n","predicted class for the next char: 11   predicted char: b':'\n","predicted class for the next char: 7   predicted char: b','\n","predicted class for the next char: 20   predicted char: b'G'\n","predicted class for the next char: 50   predicted char: b'k'\n","predicted class for the next char: 37   predicted char: b'X'\n","predicted class for the next char: 27   predicted char: b'N'\n","predicted class for the next char: 21   predicted char: b'H'\n","predicted class for the next char: 27   predicted char: b'N'\n","predicted class for the next char: 54   predicted char: b'o'\n","predicted class for the next char: 3   predicted char: b'!'\n","predicted class for the next char: 3   predicted char: b'!'\n","predicted class for the next char: 31   predicted char: b'R'\n","predicted class for the next char: 31   predicted char: b'R'\n","predicted class for the next char: 25   predicted char: b'L'\n","predicted class for the next char: 36   predicted char: b'W'\n","predicted class for the next char: 25   predicted char: b'L'\n","predicted class for the next char: 3   predicted char: b'!'\n","predicted class for the next char: 28   predicted char: b'O'\n","predicted class for the next char: 56   predicted char: b'q'\n","predicted class for the next char: 29   predicted char: b'P'\n","predicted class for the next char: 54   predicted char: b'o'\n","predicted class for the next char: 24   predicted char: b'K'\n","predicted class for the next char: 51   predicted char: b'l'\n","predicted class for the next char: 52   predicted char: b'm'\n","predicted class for the next char: 29   predicted char: b'P'\n","predicted class for the next char: 29   predicted char: b'P'\n","predicted class for the next char: 59   predicted char: b't'\n","predicted class for the next char: 7   predicted char: b','\n"]}]},{"cell_type":"markdown","source":["The next probable word would be determined from the output distribution produced by the dense layer."],"metadata":{"id":"mggq1npdJWBM"}},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_WCtAEcTHwFi","executionInfo":{"status":"ok","timestamp":1660071083254,"user_tz":-60,"elapsed":322,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"b978ea52-6e91-4bf2-ea3a-e18765028348"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"my_model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       multiple                  16896     \n","                                                                 \n"," gru (GRU)                   multiple                  3938304   \n","                                                                 \n"," dense (Dense)               multiple                  67650     \n","                                                                 \n","=================================================================\n","Total params: 4,022,850\n","Trainable params: 4,022,850\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["# Take the first sample in the batch of 64\n","print(example_batch_predictions[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Be60bPq-JjJm","executionInfo":{"status":"ok","timestamp":1660071162983,"user_tz":-60,"elapsed":554,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"501fcc32-86f4-42cd-b14f-46cc1bf3cd97"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[-0.01136525  0.00262406 -0.00616619 ... -0.00926831 -0.00483944\n","  -0.00178914]\n"," [-0.00517459 -0.00606927 -0.00573664 ... -0.00041657  0.01372316\n","  -0.01786612]\n"," [ 0.00611006 -0.00931355  0.00016606 ... -0.0089452   0.01185763\n","  -0.01367716]\n"," ...\n"," [ 0.01331605 -0.00071116  0.00900411 ... -0.0067898   0.00968277\n","   0.0017342 ]\n"," [ 0.00613625 -0.00223739  0.014972   ...  0.00111859 -0.00103418\n","  -0.01320651]\n"," [ 0.00595954  0.00593715  0.00450977 ... -0.00855094  0.00037827\n","  -0.02286581]], shape=(100, 66), dtype=float32)\n"]}]},{"cell_type":"code","source":["sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n","print(sampled_indices)"],"metadata":{"id":"-2zeVRgFKicE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n","print(sampled_indices)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lqlwq1LpKzdL","executionInfo":{"status":"ok","timestamp":1660071184066,"user_tz":-60,"elapsed":1430,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"8dc73729-7b42-481f-ea69-4683a45edcb9"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["[46 22 36 47 62 29 46 23 63 53 17 32 39 11 46  4 44 47 19 12 34 19 13 62\n"," 46 27 26 12 55  5 65  0 22 14 52 52 12 31 49 25 62 44 28 13 61 24 21 38\n"," 17 28 16 48 41 48 45 16 46 64 38 23  4 17 13 24  3 14 15 62 49 19 26 26\n"," 27 55 61 59 14 17  7 45 49 11 60 16 39 10 63 61 22 12 26 64 33 53 25 47\n","  3 63 27 53]\n"]}]},{"cell_type":"code","source":["# display the input and model prediction\n","print(\"Input:\\n\", text_from_ids(batch_feature_vector[0]).numpy())\n","print(\"Expected label: \\n\", text_from_ids(batch_label[0]).numpy())\n","print(\"Predicted label: \\n\", text_from_ids(sampled_indices).numpy())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mlt04gztLcqt","executionInfo":{"status":"ok","timestamp":1660071187552,"user_tz":-60,"elapsed":243,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"a43c0c3c-ac24-400b-b035-9505e9ae7b4a"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["Input:\n"," b'ntrance to such companions?\\nPray, get you out.\\n\\nCORIOLANUS:\\nAway!\\n\\nSecond Servingman:\\nAway! get you '\n","Expected label: \n"," b'trance to such companions?\\nPray, get you out.\\n\\nCORIOLANUS:\\nAway!\\n\\nSecond Servingman:\\nAway! get you a'\n","Predicted label: \n"," b'gIWhwPgJxnDSZ:g$ehF;UF?wgNM;p&z[UNK]IAmm;RjLweO?vKHYDOCibifCgyYJ$D?K!ABwjFMMNpvtAD,fj:uCZ3xvI;MyTnLh!xNn'\n"]}]},{"cell_type":"markdown","source":["I'm not sure why the tf.random.categorical is used here"],"metadata":{"id":"3_Qp44Xn9bXM"}},{"cell_type":"markdown","source":["## **Train the model**"],"metadata":{"id":"CE0j4P1sMim4"}},{"cell_type":"code","source":["# define a loss function for the model\n","loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n","\n","# get the loss for the first batch in the dataset\n","example_batch_mean_loss = loss(batch_label, example_batch_predictions)\n","print(f\"Prediction shape: {example_batch_predictions.shape}\")\n","print(f\"Mean loss: {example_batch_mean_loss}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AoezGOm6MqA3","executionInfo":{"status":"ok","timestamp":1660071612008,"user_tz":-60,"elapsed":244,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"c7cade03-df60-47c7-d6e0-fd1c57f800e0"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["Prediction shape: (64, 100, 66)\n","Mean loss: 4.1904191970825195\n"]}]},{"cell_type":"code","source":["tf.exp(example_batch_mean_loss).numpy()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A8fafNGJOhqB","executionInfo":{"status":"ok","timestamp":1660071621603,"user_tz":-60,"elapsed":243,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"985e1703-8191-452b-e6f2-b2a5eaf2cc00"},"execution_count":54,"outputs":[{"output_type":"execute_result","data":{"text/plain":["66.050476"]},"metadata":{},"execution_count":54}]},{"cell_type":"markdown","source":["I'm not too sure about this, but \n","- *The exponential of the mean loss should be approximately equal to the vocabulary size*. As the output logits from the dense layer should have similar magnitudes."],"metadata":{"id":"SRRYc7RlPGho"}},{"cell_type":"code","source":["# define the loss and optimizer for the model\n","model.compile(optimizer='adam', loss=loss)\n"],"metadata":{"id":"YR4tMxeqO4oH","executionInfo":{"status":"ok","timestamp":1660071634157,"user_tz":-60,"elapsed":217,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}}},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":["**Define model callbacks**\n","\n","ModelCheckpoint\n","- Saves model/ weightd at a defined frequency. (So it saves the model or it's weight at different point during training)"],"metadata":{"id":"Fki_zjlDPoql"}},{"cell_type":"code","source":["# define a directory to store checkoints of the model during training\n","checkpoint_dir = \"./training_checkpoints\"\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n","\n","# define the model checkpoint\n","# Saves the model weight at the end of each epoch to the training_checkpoints dir\n","checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n","                                                         save_weights_only=True)\n"],"metadata":{"id":"aCCPru2nPmMv","executionInfo":{"status":"ok","timestamp":1660071640411,"user_tz":-60,"elapsed":313,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}}},"execution_count":56,"outputs":[]},{"cell_type":"code","source":["# Define the training epochs \n","EPOCHS = 20"],"metadata":{"id":"LNu4wQI5R5ZV","executionInfo":{"status":"ok","timestamp":1660071651208,"user_tz":-60,"elapsed":346,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}}},"execution_count":57,"outputs":[]},{"cell_type":"code","source":["history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ykwqoDcwR_gG","executionInfo":{"status":"ok","timestamp":1660071954613,"user_tz":-60,"elapsed":301034,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"7a74c4f6-1688-4ee8-a4ab-06de2991af1a"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","172/172 [==============================] - 13s 56ms/step - loss: 2.7409\n","Epoch 2/20\n","172/172 [==============================] - 11s 56ms/step - loss: 2.0052\n","Epoch 3/20\n","172/172 [==============================] - 11s 57ms/step - loss: 1.7213\n","Epoch 4/20\n","172/172 [==============================] - 11s 57ms/step - loss: 1.5558\n","Epoch 5/20\n","172/172 [==============================] - 11s 57ms/step - loss: 1.4556\n","Epoch 6/20\n","172/172 [==============================] - 12s 57ms/step - loss: 1.3858\n","Epoch 7/20\n","172/172 [==============================] - 13s 59ms/step - loss: 1.3318\n","Epoch 8/20\n","172/172 [==============================] - 13s 58ms/step - loss: 1.2864\n","Epoch 9/20\n","172/172 [==============================] - 12s 58ms/step - loss: 1.2452\n","Epoch 10/20\n","172/172 [==============================] - 13s 58ms/step - loss: 1.2061\n","Epoch 11/20\n","172/172 [==============================] - 13s 62ms/step - loss: 1.1659\n","Epoch 12/20\n","172/172 [==============================] - 13s 61ms/step - loss: 1.1255\n","Epoch 13/20\n","172/172 [==============================] - 13s 61ms/step - loss: 1.0833\n","Epoch 14/20\n","172/172 [==============================] - 14s 61ms/step - loss: 1.0375\n","Epoch 15/20\n","172/172 [==============================] - 13s 59ms/step - loss: 0.9900\n","Epoch 16/20\n","172/172 [==============================] - 14s 60ms/step - loss: 0.9406\n","Epoch 17/20\n","172/172 [==============================] - 13s 58ms/step - loss: 0.8894\n","Epoch 18/20\n","172/172 [==============================] - 12s 57ms/step - loss: 0.8368\n","Epoch 19/20\n","172/172 [==============================] - 12s 58ms/step - loss: 0.7856\n","Epoch 20/20\n","172/172 [==============================] - 11s 57ms/step - loss: 0.7356\n"]}]},{"cell_type":"markdown","source":["## **Generating Text**\n","\n","In generating text, we would provide a seed character and then run inference to predict the next probable characther, running this multiple times would allow us to generate larger pieces of text."],"metadata":{"id":"f2Vk2rqHT-6C"}},{"cell_type":"code","source":["class OneStep(tf.keras.Model):\n","  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n","    super().__init__()\n","    self.temperature = temperature\n","    self.model = model\n","    self.chars_from_ids = chars_from_ids\n","    self.ids_from_chars = ids_from_chars\n","  \n","    # Create a mask to prevent \"[UNK]\" from being generated.\n","    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n","    sparse_mask = tf.SparseTensor(\n","        # Put a -inf at each bad index\n","        values=[-float('inf')]*len(skip_ids),\n","        indices=skip_ids,\n","        # Match the shape of the vocabulary\n","        dense_shape=[len(ids_from_chars.get_vocabulary())])\n","    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n","  \n","\n","  @tf.function\n","  def generate_one_step(self, inputs, states=None):\n","    # Convert strings to token IDs\n","    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n","    input_ids = self.ids_from_chars(input_chars).to_tensor()\n","\n","    # Run the model.\n","    # predicted_logits.shape is [batch, char, next_char_logits]\n","    predicted_logits, states = self.model(inputs=input_ids, states=states, return_state=True)\n","\n","    # Only use the last prediction.\n","    predicted_logits = predicted_logits[:, -1, :]\n","    predicted_logits = predicted_logits/self.temperature\n","    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n","    predicted_logits = predicted_logits + self.prediction_mask\n","\n","    # Sample the output logits to generate token IDs.\n","    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n","    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n","\n","    # Convert from token ids to characters\n","    predicted_chars = self.chars_from_ids(predicted_ids)\n","    \n","    # Return the characters and model state.\n","    return predicted_chars, states"],"metadata":{"id":"fIRADR6nT9vH","executionInfo":{"status":"ok","timestamp":1660072251624,"user_tz":-60,"elapsed":193,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":["one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"],"metadata":{"id":"ZfVa1MEVacKe","executionInfo":{"status":"ok","timestamp":1660072254782,"user_tz":-60,"elapsed":3,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}}},"execution_count":66,"outputs":[]},{"cell_type":"code","source":["start = time.time()\n","states = None\n","next_char = tf.constant(['Post MALONE:'])\n","result = [next_char]\n","\n","for n in range(1000):\n","  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n","  result.append(next_char)\n","\n","\n","result = tf.strings.join(result)\n","end = time.time()\n","print(result[0].numpy().decode('utf-8'), '\\n\\n', '_'*80)\n","print('\\nRun time:', end - start)"],"metadata":{"id":"BPaWE16Yajhc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660072390984,"user_tz":-60,"elapsed":3231,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"3dca3f32-92e2-4abc-920d-e9dad4c912cd"},"execution_count":71,"outputs":[{"output_type":"stream","name":"stdout","text":["Post MALONE:\n","So dreathed, cove; and, like us all,\n","What with the stroke almast came from frost\n","Against the Derint that which should be,\n","To common with thy weary maid,\n","Deny his offer; by my capiliss are\n","And frown against the wind as an end, build:\n","I will, for constant thirsy.\n","\n","CAMILLO:\n","Why 'thou're usurp the right, send for this; and now might from yourself.\n","More tears Aufidius with great reasons?\n","If is, as oncrions show humbles, and return'd\n","To see honourable stuff, a royal dead?\n","\n","GONZALO:\n","As forth shall be so. Ana, sir, my friends,\n","Will you bid great keeping I thy disgorted\n","To the mark o' the supper sevoir on.\n","Now, fellow, they do not mock itsue, for raging winter spurs,\n","Early in her, in it like an air'd\n","That Edward relish until we order our decree\n","To undestry against the heavens home to\n","die, within the heaven of my state:\n","You have us,--late you a forward but hasty porress;\n","And what I will make him to correction and her croph,\n","His city York is grant of sweet,\n","For certainty as he us not his bold co \n","\n"," ________________________________________________________________________________\n","\n","Run time: 2.962968111038208\n"]}]},{"cell_type":"markdown","source":["## **Save the text generator**"],"metadata":{"id":"qGEcOjn-EQeD"}},{"cell_type":"code","source":["# Save the model\n","tf.saved_model.save(one_step_model, 'one_step')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6VItcykdEU4B","executionInfo":{"status":"ok","timestamp":1660073165909,"user_tz":-60,"elapsed":7612,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"57a08ce6-2ff3-4fe2-9785-5adb6283f1a7"},"execution_count":83,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7f4b5b04ec10>, because it is not built.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7f4b5b04ec10>, because it is not built.\n","WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Assets written to: one_step/assets\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Assets written to: one_step/assets\n"]}]},{"cell_type":"code","source":["# load the model\n","one_step_model_reloaded = tf.saved_model.load('one_step')"],"metadata":{"id":"WQDJpvC8Ech5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Advanced: Customized Training**"],"metadata":{"id":"ye52cOY4FNVr"}},{"cell_type":"code","source":["class CustomTraining(MyModel):\n","  @tf.function\n","  def train_step(self, inputs):\n","    inputs, labels = inputs\n","    with tf.GradientTape() as tape:\n","      predictions = self(inputs, training=True)\n","      loss = self.loss(labels, predictions)\n","    grads = tape.gradient(loss, model.trainable_variables)\n","    self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n","\n","    return {'loss': loss}\n","  \n"],"metadata":{"id":"i1g79dV3EwkC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = CustomTraining(\n","    vocab_size=len(ids_from_chars.get_vocabulary()),\n","    embedding_dim=embedding_dim,\n","    rnn_units=rnn_units)"],"metadata":{"id":"K_sU4rYeGJPB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.compile(optimizer = tf.keras.optimizers.Adam(),\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"],"metadata":{"id":"0C2HLKwVGZ-Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.fit(dataset, epochs=1)"],"metadata":{"id":"NxNwo-9WGmip"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["EPOCHS = 10\n","\n","mean = tf.metrics.Mean()\n","\n","for epoch in range(EPOCHS):\n","  start = time.time()\n","\n","  mean.reset_states()\n","  for (batch_n, (inp, target)) in enumerate(dataset):\n","    logs = model.train_staep([inp, target])\n","    mean.update_state(logs['loss'])\n","\n","    if batch_n % 50 == 0:\n","      template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n","      print(template)\n","    \n","    # saving (checkpoint) the model every 5 epochs\n","    if (epoch + 1) % 5 == 0:\n","      model.save-weights(checkpoint_prefix.format(epoch=epoch))\n","    \n","\n","    print()\n","    print(f\"Epoch {epoch+1}, loss: {mean.result().numpy():.4f}\")\n","    print(f\"Time taken for 1 epoch {time.time() - start:.2f} sec\")\n","    print(\"_\"*80)"],"metadata":{"id":"lUKqcULPGuM9"},"execution_count":null,"outputs":[]}]}