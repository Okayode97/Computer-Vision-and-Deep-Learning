{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Udacity  Intro to TensorFlow for Deep Learning, Lesson 10.ipynb ","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP5euuilBjCaRkki5ea7D91"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Udacity: Intro to TensorFlow for Deep Learning**\n","## **Lesson 10 NLP: Recurrent Neural Networks**\n","\n","This lessons extends on what was covered in lesson 9. It introduces recurrent neural networks, which are able to capture temporal dependences, which are dependencies that change over time.\n","\n","This lesson covers\n","- Different RNNs\n","- Text generation using NLP models.\n"],"metadata":{"id":"CakMRRpBlwnH"}},{"cell_type":"markdown","source":["##**Simple RNNS**\n","\n","Simple recurrent neural networks, are networks which use outputs from previous time steps as additional inputs, alongside the current input.\n","\n","for example   \n","- Inputs: $X_t + Y_{t-1}$\n","- Ouput: $Y_t$\n","\n","I'm a bit effy around th calculation of the output.\n","\n","But...\n","\n","The general idea is that, previous output from the last time step are fed as additional inputs. The previous output is referred to as the state vector.\n","\n","<br>\n","\n","While simple RNN architecture are able to consider the past output when calculating it's new output, it is limited by how far back it can relate dependencies and for dependencies which occur over long period of time, a simple RNN would struggle to relate the dependencies."],"metadata":{"id":"aRKgF0f9m6_Z"}},{"cell_type":"markdown","source":["## **Long term short term Memory**\n","\n","LSTMs, were introduced to capture temporal dependencies which spam over longer periods of time. Unlike simple RNNS which have a single state vector, LSTMS have 2 state vectors, a Long term memory and a short term memory.\n","\n","<br>\n","\n","**Features of an LSTM**\n","- It's able to capture temporal dependencies spaming a long period of time\n","- It has 2 state vectors, long term and short term memory, which are used in calculating a new input\n","- LSTM feature gates: Forget, learn, remember and use gates.\n","\n","<br>\n","\n","**Workflow for an LSTM**\n","- At each time step, the LSTM has 2 state vectors: A long term memory and a short term memory\n","- The current time step input alongside the 2 state vectors are used to determine an output.\n","- The calculated output from the current time step, would be used as the short term memory for the next timestep\n","- The long term memory from the previous time step is updated:\n","  - Any pieces of the previous long term memory which is no longer relevant is removed\n","  - Any new piece of relevant information is added to the long term memory\n","\n","<br>\n","\n","**LSTM Gates**\n","- Forget gate: Determines which parts of the long term memory are no longer relevant and should be removed from memory.\n","- Learn gate: Learns new piece of informations using the current input and short term memory.\n","\n","- Remember gate: Adds any relevant information that was learnt to the long term memory. The output of this gate is the new long term memory\n","\n","- Use gate: This uses, the relevants parts of the long term memory and newly learnt information to calculate an output. The output is also used as the new short term memory for the next timestep.\n","\n"],"metadata":{"id":"sfn-5JjQrdEQ"}},{"cell_type":"markdown","source":["##**Import Dependencies**"],"metadata":{"id":"r8YH_1oFx0lD"}},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","\n","print(tf.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O-IeCkjWrcIl","executionInfo":{"status":"ok","timestamp":1656526408125,"user_tz":-60,"elapsed":3403,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"ff163a11-5fd9-4934-8bf4-96aa51d6ca38"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["2.8.2\n"]}]},{"cell_type":"markdown","source":["## **RNNs and LSTMs in code**\n","\n","The LSTM layer is within `tf.keras.Layers.LSTM` [docs](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM). likewise with the RNN layer `tf.keras.Layers.SimpleRNN`.\n","\n","<br>\n","\n","Worth noting:\n","- We can directly pass, the output of an embedding layer to an LSTM layer without adding a flatten or GlobalAverage1D layer inbetween.\n","\n","<br>\n","\n","**Further resources:**   \n","The [recurrent neural network (RNN) with keras guide](https://www.tensorflow.org/guide/keras/rnn) is a really good supplementary introduction to RNNs.\n","\n","Notes from the guide\n","- 3 built-in RNN layers: SimpleRNN, GRU, LSTM\n","- RNN can process input sequences in reverse.\n","- Feature recurrent dropout\n","- By default returns output at the last time step, but it can be configured to return a sequence instead for each time step.\n","- The layer can also be configured to return the final internal state vectors.\n","- Likewise we can also set the initial state of the RNN layer.\n","\n","\n","**Difference between layer and cell layer**\n","- \"*the RNN cell process only a single timestep*\""],"metadata":{"id":"LiptnBXSyAlA"}},{"cell_type":"markdown","source":["Simplified look at using the SimpleRNN and LSTM layers. Both these layers would take inputs from the Embedding layer, which would return a vector in n dimensions for each token in the sequence.\n","\n","To take a look at the input and given output, lets try it out on a simple dataset."],"metadata":{"id":"SD3FndIDgGNL"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"CmqDPNFLuELj","executionInfo":{"status":"ok","timestamp":1656526413032,"user_tz":-60,"elapsed":4,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}}},"outputs":[],"source":["# sample text\n","# The great pretender - The Platters\n","lyrics = [\"Oh-oh, yes, I'm the great pretender\",\n","          \"Pretending that I'm doing well\",\n","          \"My need is such I pretend too much\",\n","          \"I'm lonely, but no one can tell\",\n","          \"Oh-oh, yes, I'm the great pretender\",\n","          \"Adrift in a world of my own\",\n","          \"I've played the game but to my real shame\",\n","          \"You've left me to grieve all alone\",\n","          \"Too real is this feeling of make-believe\",\n","          \"Too real when I feel what my heart can't conceal\",\n","          \"Yes, I'm the great pretender\",\n","          \"Just laughin' and gay like a clown\",\n","          \"I seem to be what I'm not, you see\",\n","          \"I'm wearing my heart like a crown\",\n","          \"Pretending that you're still around\"\n","          \"Too real is this feeling of make-believe\",\n","          \"Too real when I feel what my heart can't conceal\",\n","          \"Yes, I'm the great pretender\",\n","          \"Just laughin' and gay like a clown\",\n","          \"I seem to be what I'm not, you see\",\n","          \"I'm wearing my heart like a crown\",\n","          \"Pretending that you're still around (still around)\"]\n"]},{"cell_type":"code","source":["# tokenize and pad the text\n","\n","# deprecated in version 2.9.1\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","\n","# define a tokenizer and fit it to the text\n","The_great_pretender = Tokenizer(num_words=150, oov_token=\"<OOV>\")\n","The_great_pretender.fit_on_texts(lyrics)\n"],"metadata":{"id":"Yies0OUAgRF2","executionInfo":{"status":"ok","timestamp":1656526417030,"user_tz":-60,"elapsed":296,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# display the word index\n","word_index = The_great_pretender.word_index\n","print(word_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RlnOAgUBjYG_","executionInfo":{"status":"ok","timestamp":1656526419502,"user_tz":-60,"elapsed":4,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"c89c8fee-e4f0-40af-e73e-197fa9760289"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["{'<OOV>': 1, \"i'm\": 2, 'my': 3, 'the': 4, 'i': 5, 'a': 6, 'real': 7, 'oh': 8, 'yes': 9, 'great': 10, 'pretender': 11, 'too': 12, 'to': 13, 'what': 14, 'heart': 15, 'like': 16, 'pretending': 17, 'that': 18, 'is': 19, 'of': 20, 'still': 21, 'but': 22, 'this': 23, 'feeling': 24, 'make': 25, 'believe': 26, 'when': 27, 'feel': 28, \"can't\": 29, 'conceal': 30, 'just': 31, \"laughin'\": 32, 'and': 33, 'gay': 34, 'clown': 35, 'seem': 36, 'be': 37, 'not': 38, 'you': 39, 'see': 40, 'wearing': 41, 'crown': 42, \"you're\": 43, 'around': 44, 'doing': 45, 'well': 46, 'need': 47, 'such': 48, 'pretend': 49, 'much': 50, 'lonely': 51, 'no': 52, 'one': 53, 'can': 54, 'tell': 55, 'adrift': 56, 'in': 57, 'world': 58, 'own': 59, \"i've\": 60, 'played': 61, 'game': 62, 'shame': 63, \"you've\": 64, 'left': 65, 'me': 66, 'grieve': 67, 'all': 68, 'alone': 69, 'aroundtoo': 70}\n"]}]},{"cell_type":"code","source":["# convert the lyrics to sequences\n","lyrics_sequence = The_great_pretender.texts_to_sequences(lyrics)\n","\n","length_of_sequence = []\n","for sequence in lyrics_sequence:\n","  print(sequence)\n","  break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i78foHRgjV5b","executionInfo":{"status":"ok","timestamp":1656526421559,"user_tz":-60,"elapsed":5,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"d20d4fa3-9069-4a8a-8dec-894126c18283"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[8, 8, 9, 2, 4, 10, 11]\n"]}]},{"cell_type":"code","source":["# display the average length of each sequence\n","length_of_sequence = []\n","for sequence in lyrics_sequence:\n","  length_of_sequence.append(len(sequence))\n","\n","\n","#src: https://www.geeksforgeeks.org/find-average-list-python/\n","def Average(lst):\n","    return sum(lst) / len(lst)\n","  \n","\n","print(Average(length_of_sequence))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gxz1XF3DkcMt","executionInfo":{"status":"ok","timestamp":1656526425421,"user_tz":-60,"elapsed":8,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"95fe6dfb-ba0a-4924-a4e9-2f8b5b8fc673"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["7.619047619047619\n"]}]},{"cell_type":"code","source":["# Apply padding to the sequences\n","lyrics_sequence_padded = pad_sequences(lyrics_sequence, maxlen=7, padding='pre',\n","                                      truncating='post')\n"],"metadata":{"id":"jjrkhRa-j2-4","executionInfo":{"status":"ok","timestamp":1656526429555,"user_tz":-60,"elapsed":367,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# define an embedding layer\n","Embedding = tf.keras.layers.Embedding(input_dim=150, output_dim=4, input_length=7)"],"metadata":{"id":"oy5CyKbMgTpF","executionInfo":{"status":"ok","timestamp":1656526431546,"user_tz":-60,"elapsed":7,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# pass in a sequence from the lyric_sequence_padded to the embedding layer\n","output = Embedding(np.array(sequence))\n","print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jLSH6BrRltrB","executionInfo":{"status":"ok","timestamp":1656526433597,"user_tz":-60,"elapsed":417,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"bfa3ee6d-eebf-45ba-b9e9-d39d5e33a8bd"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[-0.0407775  -0.0149647  -0.04853504 -0.0362067 ]\n"," [ 0.0238638   0.0244017   0.03486432 -0.03801032]\n"," [ 0.04730498  0.04166961  0.03612883  0.00908053]\n"," [-0.02445186 -0.03272854 -0.0031684  -0.04062258]\n"," [-0.03434788 -0.01567726  0.01490529  0.02566185]\n"," [-0.02445186 -0.03272854 -0.0031684  -0.04062258]\n"," [-0.03434788 -0.01567726  0.01490529  0.02566185]], shape=(7, 4), dtype=float32)\n"]}]},{"cell_type":"markdown","source":["we now have a vector of 4 dimensions representations of each tokens in the sequence.\n","\n","An intresting note (or maybe not)\n","- i don't think the embedding layer is trainable."],"metadata":{"id":"bnlNt3ZwmJaz"}},{"cell_type":"code","source":["# pass the output of the embedding layer to the LSTM layer\n","LSTM = tf.keras.layers.LSTM(units=4)\n","\n","embedding_output = Embedding(np.array([sequence]))\n","LSTM_output = LSTM(embedding_output)\n","\n","print(\"Input: {}\".format(np.array([sequence])))\n","print(\"\\nEmbedding layer output: \")\n","print(embedding_output)\n","print(\"\\nLSTM layer output: \")\n","print(LSTM_output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zTjFQHX8mx49","executionInfo":{"status":"ok","timestamp":1656526452449,"user_tz":-60,"elapsed":332,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"9d88063f-d6ea-4cb8-c92a-57cefe208142"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Input: [[17 18 43 21 44 21 44]]\n","\n","Embedding layer output: \n","tf.Tensor(\n","[[[-0.0407775  -0.0149647  -0.04853504 -0.0362067 ]\n","  [ 0.0238638   0.0244017   0.03486432 -0.03801032]\n","  [ 0.04730498  0.04166961  0.03612883  0.00908053]\n","  [-0.02445186 -0.03272854 -0.0031684  -0.04062258]\n","  [-0.03434788 -0.01567726  0.01490529  0.02566185]\n","  [-0.02445186 -0.03272854 -0.0031684  -0.04062258]\n","  [-0.03434788 -0.01567726  0.01490529  0.02566185]]], shape=(1, 7, 4), dtype=float32)\n","\n","LSTM layer output: \n","tf.Tensor([[-0.0069972   0.00251359  0.00376387 -0.00985826]], shape=(1, 4), dtype=float32)\n"]}]},{"cell_type":"markdown","source":["So what has happened??\n","- it looks like the number of units correspond to the shape of the output, so if there is 1 unit it would produce a shape of (1, 1) and if there are 4 units it would produce a shape of (1, 4).\n","- recap output of embedding layer is vector of n dimension representation of a sequence.\n","\n","<br/>\n","\n","Still doesn't answer what exactly its doing??\n","- What does the output of the LSTM layer mean??\n","\n","<br/>\n","\n","what happens if we ask it to \n","- return sequences\n","- return state\n","- go backwards"],"metadata":{"id":"V7k0DNwBpokm"}},{"cell_type":"code","source":["# lstm layer with return sequence = True\n","LSTM_return_sequence = tf.keras.layers.LSTM(units=4, return_sequences=True)\n","LSTM_return_sequence_output = LSTM_return_sequence(embedding_output)\n","\n","print(\"Input: {}\".format(np.array([sequence])))\n","print(\"\\nLSTM layer output: \")\n","print(LSTM_return_sequence_output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VnoRVg_Dpizp","executionInfo":{"status":"ok","timestamp":1656526494518,"user_tz":-60,"elapsed":336,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"eb759e8a-1fcb-4ddf-8d3a-69edccb6d146"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Input: [[17 18 43 21 44 21 44]]\n","\n","LSTM layer output: \n","tf.Tensor(\n","[[[ 0.00031957 -0.00278954  0.00191454 -0.00266983]\n","  [ 0.0010209  -0.00119254 -0.0021591   0.00193639]\n","  [ 0.00490161  0.00189113 -0.00468135  0.00442312]\n","  [-0.00146162 -0.00199073 -0.00197067  0.00299099]\n","  [-0.0057288  -0.00012202 -0.00289768  0.00381171]\n","  [-0.00944867 -0.00371482 -0.00043038  0.00346012]\n","  [-0.01174956 -0.0015525  -0.00172165  0.00493044]]], shape=(1, 7, 4), dtype=float32)\n"]}]},{"cell_type":"markdown","source":["with return sequence set to True as it iterates through the sequence, it would return a value for each timestep, as there are 7 tokens in a sequence, it returns 7 values and since we have 4 units in our LSTM for each time step it returned 4 value.\n","\n","still don't fully understand what the output mean"],"metadata":{"id":"erL0O6lqs-_D"}},{"cell_type":"code","source":["LSTM_test = tf.keras.layers.LSTM(units=8, return_sequences=True)\n","LSTM_test_output = LSTM_test(embedding_output)\n","\n","print(\"Input: {}\".format(np.array([sequence])))\n","print(\"\\nLSTM layer output: \")\n","print(LSTM_test_output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ODHbMWbJs-sO","executionInfo":{"status":"ok","timestamp":1656443918241,"user_tz":-60,"elapsed":239,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"9217e214-8b46-4bcd-b93a-7eb7b34bb0a4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Input: [[17 18 43 21 44 21 44]]\n","\n","LSTM layer output: \n","tf.Tensor(\n","[[[-4.4579613e-03  9.0390081e-03 -4.1618831e-03  3.9857998e-03\n","    1.5431750e-04  1.4170543e-03  2.6728231e-03  6.4065116e-03]\n","  [-1.8123626e-03 -1.4558776e-03  3.7191992e-03 -8.4455329e-04\n","   -3.1592435e-04 -3.7643593e-04  8.2788552e-04 -1.6677342e-03]\n","  [-7.1258042e-03  1.7895554e-03  4.1352408e-03  4.3094768e-03\n","   -5.5278647e-03 -5.9056948e-03 -1.3778711e-04 -7.5842632e-05]\n","  [-6.1017997e-04 -1.7053846e-03  4.1970001e-03 -3.5239249e-03\n","   -2.5911711e-03  3.3614601e-03  2.7934618e-03 -2.9627648e-03]\n","  [ 1.8684951e-03  3.4123105e-03 -1.4369219e-03 -2.1099832e-03\n","   -5.6553767e-03  4.1918498e-03  2.4927713e-03  1.3080460e-03]\n","  [ 5.6837257e-03 -5.2536814e-04  9.4453071e-04 -7.5894175e-03\n","   -3.3683241e-03  9.7949253e-03  4.3107201e-03 -1.8445293e-03]\n","  [ 6.3246815e-03  4.2484715e-03 -3.3057968e-03 -4.6996563e-03\n","   -6.7012394e-03  8.2459627e-03  3.3796462e-03  2.1964042e-03]]], shape=(1, 7, 8), dtype=float32)\n"]}]},{"cell_type":"markdown","source":[" ### Understanding the output of the LSTM layer\n","\n"," looking at:\n"," - https://stackoverflow.com/questions/67970519/what-does-tensorflow-lstm-return\n"," "],"metadata":{"id":"rRTL0ZN-zl3B"}},{"cell_type":"code","source":["tensor = tf.random.normal(shape=[2, 2, 2])\n","lstm = tf.keras.layers.LSTM(units=4, return_sequences=True, return_state=True)\n","result = lstm(tensor)\n","\n","print(\"Input: {}\".format(tensor))\n","print(\"result:\\n\", result )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JypRlC2L0Bnp","executionInfo":{"status":"ok","timestamp":1656529834902,"user_tz":-60,"elapsed":359,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"d274076b-2add-4d3e-fa22-91efcd2aa033"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Input: [[[-0.4942633   1.0938902 ]\n","  [-0.27461976 -0.2292373 ]]\n","\n"," [[-0.34839615 -0.39708054]\n","  [ 0.21653225  2.0741708 ]]]\n","result:\n"," [<tf.Tensor: shape=(2, 2, 4), dtype=float32, numpy=\n","array([[[ 0.01921677, -0.1382001 ,  0.02751885, -0.09191924],\n","        [-0.02658506, -0.04949056, -0.01834671, -0.04414783]],\n","\n","       [[-0.03753999,  0.06848606, -0.04679747,  0.03778585],\n","        [ 0.02192423, -0.08126742,  0.0863056 , -0.15761705]]],\n","      dtype=float32)>, <tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n","array([[-0.02658506, -0.04949056, -0.01834671, -0.04414783],\n","       [ 0.02192423, -0.08126742,  0.0863056 , -0.15761705]],\n","      dtype=float32)>, <tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n","array([[-0.05579966, -0.10042001, -0.03550037, -0.09503593],\n","       [ 0.02987111, -0.11008409,  0.26842624, -0.3075929 ]],\n","      dtype=float32)>]\n"]}]},{"cell_type":"markdown","source":["I'm sure a lot has changed since version 2.0.0 of tf. But looking at the output the dimensions are similar but with the last dimension being offset by 1.\n","\n","Looking at the answer to the question asked\n","- the first output is the output of all hidden states\n","- the second tensor is the short term memory of the neural network\n","- the third tensor is the long term memory of the neural network "],"metadata":{"id":"pIqwkZCN0Q_a"}},{"cell_type":"markdown","source":["**A quick note on the dimensions.**\n","\n","For  a given input dimension [X, Y, Z]. X would be the batch dimension and determines the number of initial element in the array. So if\n","- X = 2 we would have an array containing 2 elements. $[1, 2]$\n","- X = 5, we would have 5 elements in the array. $[1, 2, 3, 4, 5]$\n","\n","The next dimension Y would determine the number of element within the initial set of elements. so if our dimensions are\n","- [1, 2, Z], we would have an array structured like this $[ [[], []] ]$\n","Admittedly it looks confusing, but the array would contain a single element, which in turn contains 2 elements.\n","\n","stacking that further, the 3rd dimension determines the number of elements within the last set. For example an array with a dimension of\n","\n","- [1, 2, 3] could like this, [[[1, 2, 3], [4, 5, 6]]]\n","- [3, 4, 6] could look like this \n","\n","```\n","[[[1, 2, 3, 4, 5, 6], [7, 8, 9, 10, 11, 12], [13, 14, 15, 16, 17, 18], [19, 20, 21, 22, 23, 24]],\n","[[11, 12, 13, 14, 15, 16], [17, 18, 19, 20, 21, 22], [23, 24, 25, 26, 27, 28], [29, 30, 31, 32, 33, 24]],\n","[[21, 22, 23, 24, 25, 26], [27, 28, 29, 30, 31, 32], [33, 34, 35, 36, 37, 38], [39, 40, 41, 42, 43, 44]]]\n","```\n","<br>\n","\n","**Back to understanding the output of the LSTM**\n","- From my current understanding, i think only the batch dimension and the number of units in the LSTM layer would affect the dimension of the output. (provided you do not wrap the layer with a bidirectional layer).\n","- So for a given input with X batches and with A units in the layer the dimension of the output would be X, A. \n","\n","What i think occurs is that within each batch the output as the LSTM cycles through the sequence is then passed on to the next set of sequence in the batch, so for the first batch above\n","\n","```\n","[[1, 2, 3, 4, 5, 6], [7, 8, 9, 10, 11, 12], [13, 14, 15, 16, 17, 18], [19, 20, 21, 22, 23, 24]]\n","```\n","The LSTM would cycle through `[1, 2, 3, 4, 5, 6]` and then produce an output 7, which would then be used when cycling through the next sequence `[7, 8, 9, 10, 11, 12]` and again the output is passed on to the next sequence within the batch.\n"],"metadata":{"id":"CgITAORytyNU"}},{"cell_type":"code","source":["# lets try passing in the example sequence above to an LSTM layer and view what the output is\n","\n","test_sequence = np.array([[[1, 2, 3, 4, 5, 6], [7, 8, 9, 10, 11, 12], [13, 14, 15, 16, 17, 18], [19, 20, 21, 22, 23, 24]]], dtype=np.float32)\n","LSTM_test_2 = tf.keras.layers.LSTM(2)\n","\n","test_sequence_output = LSTM_test_2(test_sequence)\n","\n","print(\"Results: {}\".format(test_sequence_output))\n"],"metadata":{"id":"ypDvm68T0QsF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656528826306,"user_tz":-60,"elapsed":192,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"ffda8817-58fa-4b52-a7ba-f66fcdc867b4"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Results: [[-3.2980672e-01 -8.5024832e-09]]\n"]}]},{"cell_type":"markdown","source":["So for a single batch, it has cycled through the sequence and produced 2 seperate outputs for each unit."],"metadata":{"id":"gzpLCYWN0K6x"}},{"cell_type":"code","source":["# set go_backwards to True\n","LSTM_test_3 = tf.keras.layers.LSTM(2, go_backwards=True)\n","\n","test_sequence_output_1 = LSTM_test_3(test_sequence)\n","\n","print(\"Results: {}\".format(test_sequence_output_1))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lu9JDaxe0KcB","executionInfo":{"status":"ok","timestamp":1656529048659,"user_tz":-60,"elapsed":383,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"24bfe4ef-3e7d-4011-86ff-0208e590fd0a"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Results: [[-0.02918967  0.06942581]]\n"]}]},{"cell_type":"markdown","source":["reading the docs, it would process the sequence backwards and returns the reversed sequence."],"metadata":{"id":"3htvUv4J1Z_K"}},{"cell_type":"code","source":["# set return sequence to True\n","\n","LSTM_test_4 = tf.keras.layers.LSTM(2, return_sequences=True)\n","\n","test_sequence_output_2 = LSTM_test_4(test_sequence)\n","\n","print(\"Results: {}\".format(test_sequence_output_2))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pXdFxyZ10o62","executionInfo":{"status":"ok","timestamp":1656529360206,"user_tz":-60,"elapsed":475,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"29306c6e-9fa9-4e3e-9d04-437b08cf6b55"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Results: [[[ 4.5328138e-06 -1.6741604e-03]\n","  [ 2.3820494e-09 -1.7256550e-03]\n","  [ 1.2384534e-12 -1.8277960e-03]\n","  [ 6.4388152e-16 -1.9796446e-03]]]\n"]}]},{"cell_type":"markdown","source":["Looking at the output,\n","```\n","[[[ 4.5328138e-06 -1.6741604e-03] --> Output for the 2 units on the first sequence in the batch.\n","  [ 2.3820494e-09 -1.7256550e-03] --> Output for the 2 units on the second sequence in the batch.\n","  [ 1.2384534e-12 -1.8277960e-03] --> Output for the 2 units on the third sequence in the batch.\n","  [ 6.4388152e-16 -1.9796446e-03] --> Output for the 2 units on the fourth sequence in the batch.]]\n","```\n"],"metadata":{"id":"strt35Db269a"}},{"cell_type":"code","source":["# set return state to True\n","LSTM_test_5 = tf.keras.layers.LSTM(2, return_state=True)\n","\n","test_sequence_output_5 = LSTM_test_5(test_sequence)\n","\n","print(\"Results: {}\".format(test_sequence_output_5))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cm-CyqOi3XY3","executionInfo":{"status":"ok","timestamp":1656529969030,"user_tz":-60,"elapsed":237,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"abe37b0e-ad82-4003-81bc-16ede8986096"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Results: [<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 9.99174535e-01, -1.22137795e-11]], dtype=float32)>, <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 9.99174535e-01, -1.22137795e-11]], dtype=float32)>, <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 3.8983569e+00, -1.2234241e-11]], dtype=float32)>]\n"]}]},{"cell_type":"markdown","source":["Recap again that the LSTM has 2 states, the long term and short term states.\n","looking at the docs, for the given arguements it would return the \n","- final output\n","- final memory state --> (Long term memory)\n","- final carry state --> (Short term memory)"],"metadata":{"id":"Y2bwl_mR4sZs"}},{"cell_type":"markdown","source":["**More reading resources**\n","- https://machinelearningmastery.com/return-sequences-and-return-states-for-lstms-in-keras/"],"metadata":{"id":"nt4xKeSc5l6V"}},{"cell_type":"markdown","source":[""],"metadata":{"id":"O-zfXYxQ9Ywz"}}]}