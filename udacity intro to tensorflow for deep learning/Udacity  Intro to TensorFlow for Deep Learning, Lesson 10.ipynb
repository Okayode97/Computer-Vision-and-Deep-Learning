{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Udacity  Intro to TensorFlow for Deep Learning, Lesson 10.ipynb ","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMqwEogpgudM3Fih6SBt+wB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Udacity: Intro to TensorFlow for Deep Learning**\n","## **Lesson 10 NLP: Recurrent Neural Networks**\n","\n","This lessons extends on what was covered in lesson 9. It introduces recurrent neural networks, which are able to capture temporal dependences, which are dependencies that change over time.\n","\n","This lesson covers\n","- Different RNNs\n","- Text generation using NLP models.\n"],"metadata":{"id":"CakMRRpBlwnH"}},{"cell_type":"markdown","source":["##**Simple RNNS**\n","\n","Simple recurrent neural networks, are networks which use outputs from previous time steps as additional inputs, alongside the current input.\n","\n","for example   \n","- Inputs: $X_t + Y_{t-1}$\n","- Ouput: $Y_t$\n","\n","I'm a bit effy around th calculation of the output.\n","\n","But...\n","\n","The general idea is that, previous output from the last time step are fed as additional inputs. The previous output is referred to as the state vector.\n","\n","<br>\n","\n","While simple RNN architecture are able to consider the past output when calculating it's new output, it is limited by how far back it can relate dependencies and for dependencies which occur over long period of time, a simple RNN would struggle to relate the dependencies."],"metadata":{"id":"aRKgF0f9m6_Z"}},{"cell_type":"markdown","source":["## **Long term short term Memory**\n","\n","LSTMs, were introduced to capture temporal dependencies which spam over longer periods of time. Unlike simple RNNS which have a single state vector, LSTMS have 2 state vectors, a Long term memory and a short term memory.\n","\n","<br>\n","\n","**Features of an LSTM**\n","- It's able to capture temporal dependencies spaming a long period of time\n","- It has 2 state vectors, long term and short term memory, which are used in calculating a new input\n","- LSTM feature gates: Forget, learn, remember and use gates.\n","\n","<br>\n","\n","**Workflow for an LSTM**\n","- At each time step, the LSTM has 2 state vectors: A long term memory and a short term memory\n","- The current time step input alongside the 2 state vectors are used to determine an output.\n","- The calculated output from the current time step, would be used as the short term memory for the next timestep\n","- The long term memory from the previous time step is updated:\n","  - Any pieces of the previous long term memory which is no longer relevant is removed\n","  - Any new piece of relevant information is added to the long term memory\n","\n","<br>\n","\n","**LSTM Gates**\n","- Forget gate: Determines which parts of the long term memory are no longer relevant and should be removed from memory.\n","- Learn gate: Learns new piece of informations using the current input and short term memory.\n","\n","- Remember gate: Adds any relevant information that was learnt to the long term memory. The output of this gate is the new long term memory\n","\n","- Use gate: This uses, the relevants parts of the long term memory and newly learnt information to calculate an output. The output is also used as the new short term memory for the next timestep.\n","\n"],"metadata":{"id":"sfn-5JjQrdEQ"}},{"cell_type":"markdown","source":["##**Import Dependencies**"],"metadata":{"id":"r8YH_1oFx0lD"}},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","\n","print(tf.__version__)"],"metadata":{"id":"O-IeCkjWrcIl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **RNNs and LSTMs in code**\n","\n","The LSTM layer is within `tf.keras.Layers.LSTM` [docs](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM). likewise with the RNN layer `tf.keras.Layers.SimpleRNN`.\n","\n","<br>\n","\n","Worth noting:\n","- We can directly pass, the output of an embedding layer to an LSTM layer without adding a flatten or GlobalAverage1D layer inbetween.\n","\n","<br>\n","\n","**Further resources:**   \n","The [recurrent neural network (RNN) with keras guide](https://www.tensorflow.org/guide/keras/rnn) is a really good supplementary introduction to RNNs.\n","\n","Notes from the guide\n","- 3 built-in RNN layers: SimpleRNN, GRU, LSTM\n","- RNN can process input sequences in reverse.\n","- Feature recurrent dropout\n","- By default returns output at the last time step, but it can be configured to return a sequence instead for each time step.\n","- The layer can also be configured to return the final internal state vectors.\n","- Likewise we can also set the initial state of the RNN layer.\n","\n","\n","**Difference between layer and cell layer**\n","- \"*the RNN cell process only a single timestep*\""],"metadata":{"id":"LiptnBXSyAlA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"CmqDPNFLuELj"},"outputs":[],"source":[""]}]}