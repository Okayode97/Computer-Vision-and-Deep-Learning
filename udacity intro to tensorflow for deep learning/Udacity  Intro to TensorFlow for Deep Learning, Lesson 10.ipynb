{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Udacity  Intro to TensorFlow for Deep Learning, Lesson 10.ipynb ","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNqXDNB+pfQori+3UOqdf/E"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Udacity: Intro to TensorFlow for Deep Learning**\n","## **Lesson 10 NLP: Recurrent Neural Networks**\n","\n","This lessons extends on what was covered in lesson 9. It introduces recurrent neural networks, which are able to capture temporal dependences, which are dependencies that change over time.\n","\n","This lesson covers\n","- Different RNNs\n","- Text generation using NLP models.\n"],"metadata":{"id":"CakMRRpBlwnH"}},{"cell_type":"markdown","source":["##**Simple RNNS**\n","\n","Simple recurrent neural networks, are networks which use outputs from previous time steps as additional inputs, alongside the current input.\n","\n","for example   \n","- Inputs: $X_t + Y_{t-1}$\n","- Ouput: $Y_t$\n","\n","I'm a bit effy around th calculation of the output.\n","\n","But...\n","\n","The general idea is that, previous output from the last time step are fed as additional inputs. The previous output is referred to as the state vector.\n","\n","<br>\n","\n","While simple RNN architecture are able to consider the past output when calculating it's new output, it is limited by how far back it can relate dependencies and for dependencies which occur over long period of time, a simple RNN would struggle to relate the dependencies."],"metadata":{"id":"aRKgF0f9m6_Z"}},{"cell_type":"markdown","source":["## **Long term short term Memory**\n","\n","LSTMs, were introduced to capture temporal dependencies which spam over longer periods of time. Unlike simple RNNS which have a single state vector, LSTMS have 2 state vectors, a Long term memory and a short term memory.\n","\n","<br>\n","\n","**Features of an LSTM**\n","- It's able to capture temporal dependencies spaming a long period of time\n","- It has 2 state vectors, long term and short term memory, which are used in calculating a new input\n","- LSTM feature gates: Forget, learn, remember and use gates.\n","\n","<br>\n","\n","**Workflow for an LSTM**\n","- At each time step, the LSTM has 2 state vectors: A long term memory and a short term memory\n","- The current time step input alongside the 2 state vectors are used to determine an output.\n","- The calculated output from the current time step, would be used as the short term memory for the next timestep\n","- The long term memory from the previous time step is updated:\n","  - Any pieces of the previous long term memory which is no longer relevant is removed\n","  - Any new piece of relevant information is added to the long term memory\n","\n","<br>\n","\n","**LSTM Gates**\n","- Forget gate: Determines which parts of the long term memory are no longer relevant and should be removed from memory.\n","- Learn gate: Learns new piece of informations using the current input and short term memory.\n","\n","- Remember gate: Adds any relevant information that was learnt to the long term memory. The output of this gate is the new long term memory\n","\n","- Use gate: This uses, the relevants parts of the long term memory and newly learnt information to calculate an output. The output is also used as the new short term memory for the next timestep.\n","\n"],"metadata":{"id":"sfn-5JjQrdEQ"}},{"cell_type":"markdown","source":["##**Import Dependencies**"],"metadata":{"id":"r8YH_1oFx0lD"}},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","\n","print(tf.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O-IeCkjWrcIl","executionInfo":{"status":"ok","timestamp":1656440349595,"user_tz":-60,"elapsed":2862,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"0b34728a-6e8b-4f1f-ca91-cbc5235400ed"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["2.8.2\n"]}]},{"cell_type":"markdown","source":["## **RNNs and LSTMs in code**\n","\n","The LSTM layer is within `tf.keras.Layers.LSTM` [docs](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM). likewise with the RNN layer `tf.keras.Layers.SimpleRNN`.\n","\n","<br>\n","\n","Worth noting:\n","- We can directly pass, the output of an embedding layer to an LSTM layer without adding a flatten or GlobalAverage1D layer inbetween.\n","\n","<br>\n","\n","**Further resources:**   \n","The [recurrent neural network (RNN) with keras guide](https://www.tensorflow.org/guide/keras/rnn) is a really good supplementary introduction to RNNs.\n","\n","Notes from the guide\n","- 3 built-in RNN layers: SimpleRNN, GRU, LSTM\n","- RNN can process input sequences in reverse.\n","- Feature recurrent dropout\n","- By default returns output at the last time step, but it can be configured to return a sequence instead for each time step.\n","- The layer can also be configured to return the final internal state vectors.\n","- Likewise we can also set the initial state of the RNN layer.\n","\n","\n","**Difference between layer and cell layer**\n","- \"*the RNN cell process only a single timestep*\""],"metadata":{"id":"LiptnBXSyAlA"}},{"cell_type":"markdown","source":["Simplified look at using the SimpleRNN and LSTM layers. Both these layers would take inputs from the Embedding layer, which would return a vector in n dimensions for each token in the sequence.\n","\n","To take a look at the input and given output, lets try it out on a simple dataset."],"metadata":{"id":"SD3FndIDgGNL"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"CmqDPNFLuELj","executionInfo":{"status":"ok","timestamp":1656440276711,"user_tz":-60,"elapsed":16,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}}},"outputs":[],"source":["# sample text\n","# The great pretender - The Platters\n","lyrics = [\"Oh-oh, yes, I'm the great pretender\",\n","          \"Pretending that I'm doing well\",\n","          \"My need is such I pretend too much\",\n","          \"I'm lonely, but no one can tell\",\n","          \"Oh-oh, yes, I'm the great pretender\",\n","          \"Adrift in a world of my own\",\n","          \"I've played the game but to my real shame\",\n","          \"You've left me to grieve all alone\",\n","          \"Too real is this feeling of make-believe\",\n","          \"Too real when I feel what my heart can't conceal\",\n","          \"Yes, I'm the great pretender\",\n","          \"Just laughin' and gay like a clown\",\n","          \"I seem to be what I'm not, you see\",\n","          \"I'm wearing my heart like a crown\",\n","          \"Pretending that you're still around\"\n","          \"Too real is this feeling of make-believe\",\n","          \"Too real when I feel what my heart can't conceal\",\n","          \"Yes, I'm the great pretender\",\n","          \"Just laughin' and gay like a clown\",\n","          \"I seem to be what I'm not, you see\",\n","          \"I'm wearing my heart like a crown\",\n","          \"Pretending that you're still around (still around)\"]\n"]},{"cell_type":"code","source":["# tokenize and pad the text\n","\n","# deprecated in version 2.9.1\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","\n","# define a tokenizer and fit it to the text\n","The_great_pretender = Tokenizer(num_words=150, oov_token=\"<OOV>\")\n","The_great_pretender.fit_on_texts(lyrics)\n"],"metadata":{"id":"Yies0OUAgRF2","executionInfo":{"status":"ok","timestamp":1656440579962,"user_tz":-60,"elapsed":319,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# display the word index\n","word_index = The_great_pretender.word_index\n","print(word_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RlnOAgUBjYG_","executionInfo":{"status":"ok","timestamp":1656440627298,"user_tz":-60,"elapsed":453,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"12dfcb33-21f0-4f0c-df77-e36c087542c9"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["{'<OOV>': 1, \"i'm\": 2, 'my': 3, 'the': 4, 'i': 5, 'a': 6, 'real': 7, 'oh': 8, 'yes': 9, 'great': 10, 'pretender': 11, 'too': 12, 'to': 13, 'what': 14, 'heart': 15, 'like': 16, 'pretending': 17, 'that': 18, 'is': 19, 'of': 20, 'still': 21, 'but': 22, 'this': 23, 'feeling': 24, 'make': 25, 'believe': 26, 'when': 27, 'feel': 28, \"can't\": 29, 'conceal': 30, 'just': 31, \"laughin'\": 32, 'and': 33, 'gay': 34, 'clown': 35, 'seem': 36, 'be': 37, 'not': 38, 'you': 39, 'see': 40, 'wearing': 41, 'crown': 42, \"you're\": 43, 'around': 44, 'doing': 45, 'well': 46, 'need': 47, 'such': 48, 'pretend': 49, 'much': 50, 'lonely': 51, 'no': 52, 'one': 53, 'can': 54, 'tell': 55, 'adrift': 56, 'in': 57, 'world': 58, 'own': 59, \"i've\": 60, 'played': 61, 'game': 62, 'shame': 63, \"you've\": 64, 'left': 65, 'me': 66, 'grieve': 67, 'all': 68, 'alone': 69, 'aroundtoo': 70}\n"]}]},{"cell_type":"code","source":["# convert the lyrics to sequences\n","lyrics_sequence = The_great_pretender.texts_to_sequences(lyrics)\n","\n","length_of_sequence = []\n","for sequence in lyrics_sequence:\n","  print(sequence)\n","  break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i78foHRgjV5b","executionInfo":{"status":"ok","timestamp":1656440710909,"user_tz":-60,"elapsed":450,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"0f07e2fa-b8ac-4f7a-b68c-a6719ad6cabc"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["[8, 8, 9, 2, 4, 10, 11]\n"]}]},{"cell_type":"code","source":["# display the average length of each sequence\n","length_of_sequence = []\n","for sequence in lyrics_sequence:\n","  length_of_sequence.append(len(sequence))\n","\n","\n","#src: https://www.geeksforgeeks.org/find-average-list-python/\n","def Average(lst):\n","    return sum(lst) / len(lst)\n","  \n","\n","print(Average(length_of_sequence))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gxz1XF3DkcMt","executionInfo":{"status":"ok","timestamp":1656440956018,"user_tz":-60,"elapsed":903,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"848e0e4c-adb7-41f9-98c3-41c6326b359a"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["7.619047619047619\n"]}]},{"cell_type":"code","source":["# Apply padding to the sequences\n","lyrics_sequence_padded = pad_sequences(lyrics_sequence, maxlen=7, padding='pre',\n","                                      truncating='post')\n"],"metadata":{"id":"jjrkhRa-j2-4","executionInfo":{"status":"ok","timestamp":1656441029264,"user_tz":-60,"elapsed":705,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# define an embedding layer\n","Embedding = tf.keras.layers.Embedding(input_dim=150, output_dim=4, input_length=7)"],"metadata":{"id":"oy5CyKbMgTpF","executionInfo":{"status":"ok","timestamp":1656441195379,"user_tz":-60,"elapsed":289,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# pass an sequence from the lyric_sequence_padded to the embedding layer\n","output = Embedding(np.array(sequence))\n","print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jLSH6BrRltrB","executionInfo":{"status":"ok","timestamp":1656441305865,"user_tz":-60,"elapsed":751,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"4388d391-151a-4ae2-de47-9c12ea122a2d"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[-0.04459211  0.02571313 -0.03931668  0.02958694]\n"," [ 0.02224226 -0.04186448  0.02919896 -0.03644032]\n"," [-0.02129121 -0.0400339  -0.02416347  0.04578217]\n"," [-0.01037017  0.02882774  0.04588655 -0.04674643]\n"," [-0.01522291  0.03582403  0.01851526  0.04984926]\n"," [-0.01037017  0.02882774  0.04588655 -0.04674643]\n"," [-0.01522291  0.03582403  0.01851526  0.04984926]], shape=(7, 4), dtype=float32)\n"]}]},{"cell_type":"markdown","source":["we now have a vector of 4 dimensions representations of each tokens in the sequence.\n","\n","An intresting note (or maybe not)\n","- i don't think the embedding layer is trainable."],"metadata":{"id":"bnlNt3ZwmJaz"}},{"cell_type":"code","source":["# pass the output of the embedding layer to the LSTM layer\n","LSTM = tf.keras.layers.LSTM(units=4)\n","\n","embedding_output = Embedding(np.array([sequence]))\n","LSTM_output = LSTM(embedding_output)\n","\n","print(\"Input: {}\".format(np.array([sequence])))\n","print(\"\\nEmbedding layer output: \")\n","print(embedding_output)\n","print(\"\\nLSTM layer output: \")\n","print(LSTM_output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zTjFQHX8mx49","executionInfo":{"status":"ok","timestamp":1656442408057,"user_tz":-60,"elapsed":899,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"6c548a72-6222-4b01-d786-e6645422c6df"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Input: [[17 18 43 21 44 21 44]]\n","\n","Embedding layer output: \n","tf.Tensor(\n","[[[-0.04459211  0.02571313 -0.03931668  0.02958694]\n","  [ 0.02224226 -0.04186448  0.02919896 -0.03644032]\n","  [-0.02129121 -0.0400339  -0.02416347  0.04578217]\n","  [-0.01037017  0.02882774  0.04588655 -0.04674643]\n","  [-0.01522291  0.03582403  0.01851526  0.04984926]\n","  [-0.01037017  0.02882774  0.04588655 -0.04674643]\n","  [-0.01522291  0.03582403  0.01851526  0.04984926]]], shape=(1, 7, 4), dtype=float32)\n","\n","LSTM layer output: \n","tf.Tensor([[ 0.00769531  0.0146683   0.00121428 -0.00279331]], shape=(1, 4), dtype=float32)\n"]}]},{"cell_type":"markdown","source":["So what has happened??\n","- it looks like the number of units correspond to the shape of the output, so if there is 1 unit it would produce a shape of (1, 1) and if there are 4 units it would produce a shape of (1, 4).\n","- recap output of embedding layer is vector of n dimension representation of a sequence.\n","\n","<br/>\n","\n","Still doesn't answer what exactly its doing??\n","- What does the output of the LSTM layer mean??\n","\n","<br/>\n","\n","what happens if we ask it to \n","- return sequences\n","- return state\n","- go backwards"],"metadata":{"id":"V7k0DNwBpokm"}},{"cell_type":"code","source":["# lstm layer with return sequence = True\n","LSTM_return_sequence = tf.keras.layers.LSTM(units=4, return_sequences=True)\n","LSTM_return_sequence_output = LSTM_return_sequence(embedding_output)\n","\n","print(\"Input: {}\".format(np.array([sequence])))\n","print(\"\\nLSTM layer output: \")\n","print(LSTM_return_sequence_output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VnoRVg_Dpizp","executionInfo":{"status":"ok","timestamp":1656443075962,"user_tz":-60,"elapsed":772,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"5b873dd3-40eb-4b36-9ffe-8996ae8f1606"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Input: [[17 18 43 21 44 21 44]]\n","\n","LSTM layer output: \n","tf.Tensor(\n","[[[ 1.9649762e-05  2.0224554e-03  8.7440340e-03  4.0050391e-03]\n","  [ 2.9053944e-03  3.3087025e-03 -3.7620508e-04 -3.2723253e-03]\n","  [ 9.8999811e-04  1.3154540e-02 -5.6852326e-03  1.3051000e-03]\n","  [ 6.3451435e-03  8.1153130e-03  2.1200352e-03 -3.3731095e-04]\n","  [ 2.1511361e-03  9.9434238e-03  2.2941262e-03  9.3798516e-03]\n","  [ 7.0728022e-03  4.6256278e-03  7.7789957e-03  4.5243585e-03]\n","  [ 2.7511823e-03  6.8042660e-03  6.2713060e-03  1.2181924e-02]]], shape=(1, 7, 4), dtype=float32)\n"]}]},{"cell_type":"markdown","source":["with return sequence set to True as it iterates through the sequence, it would return a value for each timestep, as there are 7 tokens in a sequence, it returns 7 values and since we have 4 units in our LSTM for each time step it returned 4 value.\n","\n","still don't fully understand what the output mean"],"metadata":{"id":"erL0O6lqs-_D"}},{"cell_type":"code","source":["LSTM_test = tf.keras.layers.LSTM(units=8, return_sequences=True)\n","LSTM_test_output = LSTM_test(embedding_output)\n","\n","print(\"Input: {}\".format(np.array([sequence])))\n","print(\"\\nLSTM layer output: \")\n","print(LSTM_test_output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ODHbMWbJs-sO","executionInfo":{"status":"ok","timestamp":1656443918241,"user_tz":-60,"elapsed":239,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"9217e214-8b46-4bcd-b93a-7eb7b34bb0a4"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Input: [[17 18 43 21 44 21 44]]\n","\n","LSTM layer output: \n","tf.Tensor(\n","[[[-4.4579613e-03  9.0390081e-03 -4.1618831e-03  3.9857998e-03\n","    1.5431750e-04  1.4170543e-03  2.6728231e-03  6.4065116e-03]\n","  [-1.8123626e-03 -1.4558776e-03  3.7191992e-03 -8.4455329e-04\n","   -3.1592435e-04 -3.7643593e-04  8.2788552e-04 -1.6677342e-03]\n","  [-7.1258042e-03  1.7895554e-03  4.1352408e-03  4.3094768e-03\n","   -5.5278647e-03 -5.9056948e-03 -1.3778711e-04 -7.5842632e-05]\n","  [-6.1017997e-04 -1.7053846e-03  4.1970001e-03 -3.5239249e-03\n","   -2.5911711e-03  3.3614601e-03  2.7934618e-03 -2.9627648e-03]\n","  [ 1.8684951e-03  3.4123105e-03 -1.4369219e-03 -2.1099832e-03\n","   -5.6553767e-03  4.1918498e-03  2.4927713e-03  1.3080460e-03]\n","  [ 5.6837257e-03 -5.2536814e-04  9.4453071e-04 -7.5894175e-03\n","   -3.3683241e-03  9.7949253e-03  4.3107201e-03 -1.8445293e-03]\n","  [ 6.3246815e-03  4.2484715e-03 -3.3057968e-03 -4.6996563e-03\n","   -6.7012394e-03  8.2459627e-03  3.3796462e-03  2.1964042e-03]]], shape=(1, 7, 8), dtype=float32)\n"]}]},{"cell_type":"markdown","source":[" ### Understanding the output of the LSTM layer\n","\n"," looking at:\n"," - https://stackoverflow.com/questions/67970519/what-does-tensorflow-lstm-return\n"," "],"metadata":{"id":"rRTL0ZN-zl3B"}},{"cell_type":"code","source":["tensor = tf.random.normal( shape = [ 2, 2, 2 ])\n","lstm = tf.keras.layers.LSTM(units=4, return_sequences=True, return_state=True )\n","result = lstm(tensor)\n","print( \"result:\\n\", result )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JypRlC2L0Bnp","executionInfo":{"status":"ok","timestamp":1656445003131,"user_tz":-60,"elapsed":536,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"470e191a-8fbd-4c45-ca8f-703b0f1dfcf8"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["result:\n"," [<tf.Tensor: shape=(2, 2, 4), dtype=float32, numpy=\n","array([[[-0.06319275, -0.01700984,  0.05514352, -0.05898715],\n","        [-0.06919891,  0.03776611,  0.27123466, -0.2143757 ]],\n","\n","       [[-0.11517125, -0.06272746, -0.04737367, -0.00527546],\n","        [-0.0422636 , -0.17164788, -0.20939562,  0.07271944]]],\n","      dtype=float32)>, <tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n","array([[-0.06919891,  0.03776611,  0.27123466, -0.2143757 ],\n","       [-0.0422636 , -0.17164788, -0.20939562,  0.07271944]],\n","      dtype=float32)>, <tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n","array([[-0.08944879,  0.06403987,  0.5540789 , -0.33707172],\n","       [-0.13707983, -0.3527763 , -0.42899728,  0.18073909]],\n","      dtype=float32)>]\n"]}]},{"cell_type":"markdown","source":["I'm sure a lot has changed since version 2.0.0 of tf. But looking at the output the dimensions are similar but with the last dimension being offset by 1.\n","\n","Looking at the answer to the question asked\n","- the first output is the output of all hidden states\n","- the second tensor is the short term memory of the neural network\n","- the third tensor is the long term memory of the neural network "],"metadata":{"id":"pIqwkZCN0Q_a"}},{"cell_type":"code","source":[""],"metadata":{"id":"ypDvm68T0QsF"},"execution_count":null,"outputs":[]}]}