{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Udacity: Intro to TensorFlow for Deep Learning, Lesson 9.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNeR3MVTqUCcq/1NAWtS3qD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Udacity: Intro to TensorFlow for Deep Learning**\n","### **Lesson 9 NLP: Tokenization and Embeddings**\n","\n","**Introduction to NLP**   \n","This lesson was an introduction to natural language processing, which involves analysing the meaning of text and speech data. Applications of NLP include\n","- Dictation and translation\n","- Sentiment analysis\n","- Text and speech generation\n","\n","We have also seen NLP been applied in commerical products like voice assistants and other smart devices.\n","\n","</br>\n","\n","This lesson focuses on how we can prepare text data for our NLP models"],"metadata":{"id":"IhI_dNSo8F6F"}},{"cell_type":"markdown","source":["## **Preparing Text for Natural language models**\n","\n","Towards preparing text for NLP models we would need to\n","- **Tokenize the text**, which involves assigning a numerical value to words in the text training dataset\n","- We typically work with sentences and not just individual words in NLP, so after tokenization, we can then **convert sentences into sequences**.\n","- As we can have sentences of different lengths, we would also have sequences of different length, to work around this we would apply **padding and truncating** to ensure all the sequences have equal length.\n","\n","<br/>\n","\n","Other things and parameters to account for   \n","- Tokenization is only applied once and with the training set, any new words encountered in the test set would be represented with an **Out of vocabulary token, OOV**.\n","- We can define the **number of words which would get tokenized**.\n","- We can vary the **length of our sequences**, using padding and truncating.\n","- we can define **where padding and truncating is applied**, at the start or end of sequences."],"metadata":{"id":"PlKd9vMMkTp3"}},{"cell_type":"markdown","source":["## **Import required packages**"],"metadata":{"id":"QsX4JO78m16q"}},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","print(tf.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gfjrSx_6m6N9","executionInfo":{"status":"ok","timestamp":1655552422777,"user_tz":-60,"elapsed":5725,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"51999be9-5d78-4ec5-b9ef-082a66356d13"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2.8.2\n"]}]},{"cell_type":"markdown","source":["## **Tokenization**\n","\n","Lets use text from Green Eggs and Ham by Dr. Seuss.\n","\n"],"metadata":{"id":"WpS6YZAhmqNW"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"FNlC90jm7wgh"},"outputs":[],"source":["# define a list of sentences\n","Green_eggs_and_ham = [\"I AM SAM. I AM SAM. SAM I AM.\",\n","                      \"THAT SAM-I-AM! THAT SAM-I-AM! I DO NOT LIKE THAT SAM-I-AM!\",\n","                      \"DO WOULD YOU LIKE GREEN EGGS AND HAM?\",\n","                      \"I DO NOT LIKE THEM,SAM-I-AM.\",\n","                      \"I DO NOT LIKE GREEN EGGS AND HAM.\",\n","                      \"WOULD YOU LIKE THEM HERE OR THERE?\",\n","                      \"I WOULD NOT LIKE THEM HERE OR THERE.\",\n","                      \"I WOULD NOT LIKE THEM ANYWHERE.\",\n","                      \"I DO NOT LIKE GREEN EGGS AND HAM.\",\n","                      \"I DO NOT LIKE THEM, SAM-I-AM.\",\n","\n","                      \"WOULD YOU LIKE THEM IN A HOUSE?\",\n","                      \"WOULD YOU LIKE THEN WITH A MOUSE?\",\n","\n","                      \"I DO NOT LIKE THEM IN A HOUSE.\",\n","                      \"I DO NOT LIKE THEM WITH A MOUSE.\",\n","                      \"I DO NOT LIKE THEM HERE OR THERE.\",\n","                      \"I DO NOT LIKE THEM ANYWHERE.\",\n","                      \"I DO NOT LIKE GREEN EGGS AND HAM.\",\n","                      \"I DO NOT LIKE THEM, SAM-I-AM.\",\n","\n","                      \"WOULD YOU EAT THEM IN A BOX?\",\n","                      \"WOULD YOU EAT THEM WITH A FOX?\",\n","\n","                      \"NOT IN A BOX. NOT WITH A FOX.\",\n","                      \"NOT IN A HOUSE. NOT WITH A MOUSE.\",\n","                      \"I WOULD NOT EAT THEM HERE OR THERE.\",\n","                      \"I WOULD NOT EAT THEM ANYWHERE.\",\n","                      \"I WOULD NOT EAT GREEN EGGS AND HAM.\",\n","                      \"I DO NOT LIKE THEM, SAM-I-AM.\",\n","\n","                      \"WOULD YOU? COULD YOU? IN A CAR?\",\n","                      \"EAT THEM! EAT THEM! HERE THEY ARE.\",\n","\n","                      \"I WOULD NOT, COULD NOT, IN A CAR.\",\n","\n","                      \"YOU MAY LIKE THEM. YOU WILL SEE.\",\n","                      \"YOU MAY LIKE THEM IN A TREE!\",\n","\n","                      \"I WOULD NOT, COULD NOT IN A TREE.\",\n","                      \"NOT IN A CAR! YOU LET ME BE.\",\n","                      \"I DO NOT LIKE THEM IN A BOX.\",\n","                      \"I DO NOT LIKE THEM WITH A FOX.\",\n","                      \"I DO NOT LIKE THEM IN A HOUSE.\",\n","                      \"I DO NOT LIKE THEM WITH A MOUSE.\",\n","                      \"I DO NOT LIKE THEM HERE OR THERE.\",\n","                      \"I DO NOT LIKE THEM ANYWHERE.\",\n","                      \"I DO NOT LIKE GREEN EGGS AND HAM.\",\n","                      \"I DO NOT LIKE THEM, SAM-I-AM.\",\n","\n","                      \"A TRAIN! A TRAIN! A TRAIN! A TRAIN!\",\n","                      \"COULD YOU, WOULD YOU ON A TRAIN?\",\n","\n","                      \"NOT ON TRAIN! NOT IN A TREE!\",\n","                      \"NOT IN A CAR! SAM! LET ME BE!\",\n","                      \"I WOULD NOT, COULD NOT, IN A BOX.\",\n","                      \"I WOULD NOT, COULD NOT, WITH A FOX.\",\n","                      \"I WILL NOT EAT THEM IN A HOUSE.\",\n","                      \"I WILL NOT EAT THEM HERE OR THERE.\",\n","                      \"I WILL NOT EAT THEM ANYWHERE.\",\n","                      \"I DO NOT EAT GREEM EGGS AND HAM.\",\n","                      \"I DO NOT LIKE THEM, SAM-I-AM.\",\n","]"]},{"cell_type":"markdown","source":["**Defining and fitting the tokenizer and word index**"],"metadata":{"id":"ayr0EE-PzqS_"}},{"cell_type":"code","source":["# initialize the tokenizer, it would tokenize the 100 most common words in the list of sentences\n","green_eggs_and_ham_tokenizer = Tokenizer(num_words=100, oov_token=\"<OOV>\")\n","\n","# fit the tokenizer to the list of sentences\n","green_eggs_and_ham_tokenizer.fit_on_texts(Green_eggs_and_ham)\n"],"metadata":{"id":"3v3fAC-KrV5P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["View the word index for the tokenizer"],"metadata":{"id":"h77kjwN7r-Tf"}},{"cell_type":"code","source":["word_index = green_eggs_and_ham_tokenizer.word_index\n","print(word_index)\n","print(\"Number of tokenized words: {}\".format(len(word_index)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UbCp12Uvshjj","executionInfo":{"status":"ok","timestamp":1655553961055,"user_tz":-60,"elapsed":323,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"bc697a51-20f9-436f-e180-3ba3cb89a991"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'<OOV>': 1, 'i': 2, 'not': 3, 'them': 4, 'like': 5, 'a': 6, 'do': 7, 'would': 8, 'in': 9, 'you': 10, 'sam': 11, 'am': 12, 'eat': 13, 'with': 14, 'eggs': 15, 'and': 16, 'ham': 17, 'here': 18, 'green': 19, 'or': 20, 'there': 21, 'could': 22, 'train': 23, 'anywhere': 24, 'house': 25, 'mouse': 26, 'box': 27, 'fox': 28, 'car': 29, 'will': 30, 'that': 31, 'tree': 32, 'may': 33, 'let': 34, 'me': 35, 'be': 36, 'on': 37, 'then': 38, 'they': 39, 'are': 40, 'see': 41, 'greem': 42}\n","Number of tokenized words: 42\n"]}]},{"cell_type":"markdown","source":["An intresting note,\n","- it hasn't picked on any of the exclamation marks, commas or periods in the sentences.\n","- it also has 42 words in its word index and not 100, so i'm guess that it only found 42 words which were very common in the entire list and it could not find 58 other words which were also very common\n","\n","\n","</br>\n","\n","some questions to raise\n","- How often does the word have to come up in the sentence for it to be added to the word index, or does it just find the frequency of each word and then sort and filter based on frequency and desired number of words.\n"],"metadata":{"id":"EfP7L9nls-qM"}},{"cell_type":"markdown","source":["**Convert text to sequences**"],"metadata":{"id":"Wpwiot4rzyl5"}},{"cell_type":"code","source":["# get a single sentence from the list of sentences\n","sample_sentence = Green_eggs_and_ham[0]\n","print(sample_sentence)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1PBWkd9HxfNy","executionInfo":{"status":"ok","timestamp":1655555132687,"user_tz":-60,"elapsed":308,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"89100f0e-d4b5-4a07-ba4f-629396867de8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["I AM SAM. I AM SAM. SAM I AM.\n"]}]},{"cell_type":"code","source":["# convert the sentence into a sequence\n","green_eggs_and_ham_tokenizer.texts_to_sequences([sample_sentence])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3ODlErMnxpJh","executionInfo":{"status":"ok","timestamp":1655555209241,"user_tz":-60,"elapsed":317,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"07aef6f6-63f4-48c3-a661-27602bd902ff"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[2, 12, 11, 2, 12, 11, 11, 2, 12]]"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["# get more sentences from the list and convert it into sequences\n","\n","sample_sentences = Green_eggs_and_ham[0:10]\n","sample_sequences = green_eggs_and_ham_tokenizer.texts_to_sequences(sample_sentences)\n","\n","for text, sequence in zip(sample_sentences, sample_sequences):\n","  print(\"Text: {}\".format(text))\n","  print(\"Sequence: {}\".format(sequence))\n","  print(\"\\n\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ud13P_llyAiW","executionInfo":{"status":"ok","timestamp":1655555356226,"user_tz":-60,"elapsed":319,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"b1a617b4-1320-473c-a262-e13a5e14cd9f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Text: I AM SAM. I AM SAM. SAM I AM.\n","Sequence: [2, 12, 11, 2, 12, 11, 11, 2, 12]\n","\n","\n","Text: THAT SAM-I-AM! THAT SAM-I-AM! I DO NOT LIKE THAT SAM-I-AM!\n","Sequence: [31, 11, 2, 12, 31, 11, 2, 12, 2, 7, 3, 5, 31, 11, 2, 12]\n","\n","\n","Text: DO WOULD YOU LIKE GREEN EGGS AND HAM?\n","Sequence: [7, 8, 10, 5, 19, 15, 16, 17]\n","\n","\n","Text: I DO NOT LIKE THEM,SAM-I-AM.\n","Sequence: [2, 7, 3, 5, 4, 11, 2, 12]\n","\n","\n","Text: I DO NOT LIKE GREEN EGGS AND HAM.\n","Sequence: [2, 7, 3, 5, 19, 15, 16, 17]\n","\n","\n","Text: WOULD YOU LIKE THEM HERE OR THERE?\n","Sequence: [8, 10, 5, 4, 18, 20, 21]\n","\n","\n","Text: I WOULD NOT LIKE THEM HERE OR THERE.\n","Sequence: [2, 8, 3, 5, 4, 18, 20, 21]\n","\n","\n","Text: I WOULD NOT LIKE THEM ANYWHERE.\n","Sequence: [2, 8, 3, 5, 4, 24]\n","\n","\n","Text: I DO NOT LIKE GREEN EGGS AND HAM.\n","Sequence: [2, 7, 3, 5, 19, 15, 16, 17]\n","\n","\n","Text: I DO NOT LIKE THEM, SAM-I-AM.\n","Sequence: [2, 7, 3, 5, 4, 11, 2, 12]\n","\n","\n"]}]},{"cell_type":"markdown","source":["Side note\n","- The texts_to_sequences function takes in a list of texts, so we can not just pass in raw stings"],"metadata":{"id":"NjLSu-9S0rjM"}},{"cell_type":"markdown","source":["**Closer look at the Tokenizers Doc**\n","\n","[Tokenizer Doc](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)\n","\n","<br/>\n","\n","**tf.keras.preprocessing is deprecated**   \n","It looks like keras.preprocessing module is deprecated in version 2.9.1, it is now suggested to use the \n","*tf.keras.utils.text_dataset_from_directory* and *tf.keras.layers.TextVectorization* for preprocessing text input.\n","\n","For the sake of continuity with the course, i will follow along with using the depreciated code, but i'd still checkout the new methods for preprocessing text in another notebook.\n","\n","<br/>\n","\n","**Other methods with the Text tokenizer**\n","- fit_on_sequence / texts\n","- sequence to matrix / texts and text generators\n","- text to matrix / sequnce and sequence generators\n","- get_config\n","- to_json\n","\n","<br/>\n","\n","With going into too much detail on each function it seems like not only can we convert text into sequences, we can also convert them into matrix and we can only fit to sequences.\n"],"metadata":{"id":"bvhAUU51u-Sq"}},{"cell_type":"markdown","source":["## **Padding and Truncating**\n","\n","Side note again\n","- Looking at the documentation for version 2.9.1, pad_sequences has been moved to *tf.keras.utils.pad_sequences*\n","\n","<br/>\n","\n","Default behaviours\n","- Padding and truncating is applied at the start of the sequences\n","- max_length of sequence is decided by the longest sequence in the list, unless specified in the class constructor.\n","- padding value is 0\n","\n","<br/>\n","\n","i'd still try to see if i can use pad_sequence which i thought was imported from *tf.keras.preprocessing.sequence*.\n","\n","\n","\n"],"metadata":{"id":"sLbOCDbC07MB"}},{"cell_type":"code","source":["# lets take sample_sequences\n","for sequence in sample_sequences:\n","    print(sequence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L2qV-62X1-TV","executionInfo":{"status":"ok","timestamp":1655556389643,"user_tz":-60,"elapsed":296,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"58948cc6-ab93-4d38-9d23-78f1a0ac08bc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[2, 12, 11, 2, 12, 11, 11, 2, 12]\n","[31, 11, 2, 12, 31, 11, 2, 12, 2, 7, 3, 5, 31, 11, 2, 12]\n","[7, 8, 10, 5, 19, 15, 16, 17]\n","[2, 7, 3, 5, 4, 11, 2, 12]\n","[2, 7, 3, 5, 19, 15, 16, 17]\n","[8, 10, 5, 4, 18, 20, 21]\n","[2, 8, 3, 5, 4, 18, 20, 21]\n","[2, 8, 3, 5, 4, 24]\n","[2, 7, 3, 5, 19, 15, 16, 17]\n","[2, 7, 3, 5, 4, 11, 2, 12]\n"]}]},{"cell_type":"code","source":["# apply padding to the sequence to make it have a length of 10\n","padded_sample_sequences = pad_sequences(sample_sequences, maxlen=10,\n","                                        padding='pre', truncating='post')\n","\n","for sequence in padded_sample_sequences:\n","  print(sequence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RtiApV2h2XMA","executionInfo":{"status":"ok","timestamp":1655556577508,"user_tz":-60,"elapsed":359,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"44cff235-23fa-42b1-9f7a-30df02f16449"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[ 0  2 12 11  2 12 11 11  2 12]\n","[31 11  2 12 31 11  2 12  2  7]\n","[ 0  0  7  8 10  5 19 15 16 17]\n","[ 0  0  2  7  3  5  4 11  2 12]\n","[ 0  0  2  7  3  5 19 15 16 17]\n","[ 0  0  0  8 10  5  4 18 20 21]\n","[ 0  0  2  8  3  5  4 18 20 21]\n","[ 0  0  0  0  2  8  3  5  4 24]\n","[ 0  0  2  7  3  5 19 15 16 17]\n","[ 0  0  2  7  3  5  4 11  2 12]\n"]}]},{"cell_type":"markdown","source":["That seems straightforward, i've applied pre padding, so that padding is applied at the start of the sequence and i used post truncating to truncate from the end of the sequence."],"metadata":{"id":"zx7gv81b3ONA"}},{"cell_type":"markdown","source":["## **Word Embeddings**\n","\n","Word embeddings, represent individual words in our tokenized vocabulary as vectors in an n dimensional space.\n","\n","<br/>\n","\n","What does this mean?   \n","- a word would be represented as a vector of n dimensions\n","- Example if we have 5 dimensions, a word like \"Ham\" would be represented as [0, 4, 2, 9, 10]\n","\n","\n","<br/>\n","\n","My issue with this is that, why i would not pretend to fully understand Embeddings, does it not make the whole process of tokenization and sampling obsolete?, if we would simply end up with words converted into vectors of n dimension, Why go through the steps of 2 previous steps??\n","\n","\n"],"metadata":{"id":"PMyzsubu9lVJ"}},{"cell_type":"markdown","source":["Link to the [Embeddings Doc](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding)"],"metadata":{"id":"UIap8whUELke"}},{"cell_type":"code","source":["# import the Embeddings layer\n","from tensorflow.keras.layers import Embedding\n","\n"],"metadata":{"id":"pj_TKg4sCwaY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define an Embeddings layer, which returns a vector represented in 10 dimensions.\n","\n","green_eggs_and_ham_embedding_layer = Embedding(input_dim = 100, output_dim = 10, input_length=10)"],"metadata":{"id":"muDsN9ljDZpj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["lets try out the embedding layer on some of the sequence and view the output"],"metadata":{"id":"G2xN5RMLEll5"}},{"cell_type":"code","source":["one_padded_sequence = padded_sample_sequences[0]\n","print(one_padded_sequence)\n","\n","output = green_eggs_and_ham_embedding_layer(one_padded_sequence)\n","print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Na1MQUhSEpmv","executionInfo":{"status":"ok","timestamp":1655560228724,"user_tz":-60,"elapsed":567,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"3a63b028-af91-4887-fdbd-7a365e40ba6a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[ 0  2 12 11  2 12 11 11  2 12]\n","tf.Tensor(\n","[[-0.03021989  0.01281465 -0.04905542 -0.04851932 -0.02706047  0.04161533\n","   0.00115458 -0.04174959 -0.010993   -0.04647777]\n"," [ 0.03158594  0.02095946  0.03878237  0.04323217 -0.0496304  -0.0044466\n","  -0.01157327  0.01833392  0.00045573  0.00308537]\n"," [-0.03898587  0.03261942  0.00892892  0.03284807 -0.03724285  0.02448977\n","  -0.03382075 -0.01881353  0.03247478  0.01001913]\n"," [-0.01517288  0.03596203 -0.00664157  0.00320182  0.00103928 -0.03119476\n","   0.02087155  0.00902732 -0.01722797  0.02964315]\n"," [ 0.03158594  0.02095946  0.03878237  0.04323217 -0.0496304  -0.0044466\n","  -0.01157327  0.01833392  0.00045573  0.00308537]\n"," [-0.03898587  0.03261942  0.00892892  0.03284807 -0.03724285  0.02448977\n","  -0.03382075 -0.01881353  0.03247478  0.01001913]\n"," [-0.01517288  0.03596203 -0.00664157  0.00320182  0.00103928 -0.03119476\n","   0.02087155  0.00902732 -0.01722797  0.02964315]\n"," [-0.01517288  0.03596203 -0.00664157  0.00320182  0.00103928 -0.03119476\n","   0.02087155  0.00902732 -0.01722797  0.02964315]\n"," [ 0.03158594  0.02095946  0.03878237  0.04323217 -0.0496304  -0.0044466\n","  -0.01157327  0.01833392  0.00045573  0.00308537]\n"," [-0.03898587  0.03261942  0.00892892  0.03284807 -0.03724285  0.02448977\n","  -0.03382075 -0.01881353  0.03247478  0.01001913]], shape=(10, 10), dtype=float32)\n"]}]},{"cell_type":"markdown","source":["It looks like it has done exactly what we want it to do. For each word (token) in the sequence, it has created a vector represented in 10 dimensions.\n","\n","Are the vector representation of each token consistent??   \n","- yes they are, check token 2, 12 & 11. it's consistent"],"metadata":{"id":"p6xFf9phFIyI"}},{"cell_type":"markdown","source":["**Using the embeddings layer with flatten and globalAverage pooling**"],"metadata":{"id":"LgRbNELRF_Wz"}},{"cell_type":"code","source":["from tensorflow.keras.layers import Flatten, GlobalAveragePooling1D\n"],"metadata":{"id":"TLn2-bebF3-A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def embeddings_with_flatten(sequence):\n","  \"\"\" Output result of calling embedding with flatten layer.\"\"\"\n","  embedding_output = Embedding(input_dim=100, output_dim=10, input_length=10)(sequence)\n","  return Flatten()(embedding_output)\n"],"metadata":{"id":"3qPN4xQXGMe0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# calling embeddings with flatten\n","output_1 = embeddings_with_flatten(one_padded_sequence)\n","print(output_1)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lwPWwpBqHfQ1","executionInfo":{"status":"ok","timestamp":1655561015511,"user_tz":-60,"elapsed":223,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"ceddee18-ac99-48ad-e919-34e0f1b5836f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[ 0.01029031  0.04661094  0.023269    0.00515274  0.04184343  0.00934483\n","   0.04358728 -0.04875825  0.01074376 -0.039929  ]\n"," [-0.00071131 -0.0477136   0.02663224  0.02124919 -0.03404929 -0.0209569\n","  -0.00733783 -0.01769852 -0.02028185 -0.0470591 ]\n"," [-0.03174638 -0.04209901 -0.02665803  0.00995724 -0.02815077 -0.02901781\n","   0.04438217  0.03840314 -0.04537207  0.04851108]\n"," [-0.03818735  0.00078509  0.04093063  0.0094205   0.01967049  0.03563208\n","   0.03785429 -0.03872411 -0.0254331  -0.03612403]\n"," [-0.00071131 -0.0477136   0.02663224  0.02124919 -0.03404929 -0.0209569\n","  -0.00733783 -0.01769852 -0.02028185 -0.0470591 ]\n"," [-0.03174638 -0.04209901 -0.02665803  0.00995724 -0.02815077 -0.02901781\n","   0.04438217  0.03840314 -0.04537207  0.04851108]\n"," [-0.03818735  0.00078509  0.04093063  0.0094205   0.01967049  0.03563208\n","   0.03785429 -0.03872411 -0.0254331  -0.03612403]\n"," [-0.03818735  0.00078509  0.04093063  0.0094205   0.01967049  0.03563208\n","   0.03785429 -0.03872411 -0.0254331  -0.03612403]\n"," [-0.00071131 -0.0477136   0.02663224  0.02124919 -0.03404929 -0.0209569\n","  -0.00733783 -0.01769852 -0.02028185 -0.0470591 ]\n"," [-0.03174638 -0.04209901 -0.02665803  0.00995724 -0.02815077 -0.02901781\n","   0.04438217  0.03840314 -0.04537207  0.04851108]], shape=(10, 10), dtype=float32)\n"]}]},{"cell_type":"code","source":["# define the function\n","def embeddings_with_globalaveragepool(sequence):\n","  embedding_output = Embedding(input_dim=100, output_dim=10, input_length=10)(sequence)\n","  print(embedding_output)\n","  reshaped_embedding = tf.keras.layers.Reshape((10, 1))(embedding_output)\n","  print(reshaped_embedding)\n","  return GlobalAveragePooling1D()(reshaped_embedding)\n","\n","# callings with embeddings with global average pooling\n","output_2 = embeddings_with_globalaveragepool(one_padded_sequence)\n","print(output_2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b-j0oHQIIET6","executionInfo":{"status":"ok","timestamp":1655562370256,"user_tz":-60,"elapsed":215,"user":{"displayName":"Kayode Sonaike","userId":"14696411076652260026"}},"outputId":"7914a375-a3de-4ccc-9db1-796bdb0cc3d8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[ 0.02531358  0.02799585 -0.01580938  0.01671995 -0.02855491  0.03134619\n","  -0.02139058 -0.01615366 -0.02857158 -0.01546197]\n"," [ 0.04141453 -0.03586144  0.04468853  0.00243197  0.0425258   0.02570314\n","   0.03169549 -0.00939401 -0.00173203  0.02032695]\n"," [ 0.00069119  0.0211736  -0.04973649  0.00702355 -0.03485416 -0.00299828\n","   0.02871437 -0.04768872  0.00744121 -0.02722271]\n"," [-0.01300015  0.03453108  0.04390124 -0.02796248 -0.01919132  0.0014916\n","   0.03302205  0.0110528   0.02355763  0.03311569]\n"," [ 0.04141453 -0.03586144  0.04468853  0.00243197  0.0425258   0.02570314\n","   0.03169549 -0.00939401 -0.00173203  0.02032695]\n"," [ 0.00069119  0.0211736  -0.04973649  0.00702355 -0.03485416 -0.00299828\n","   0.02871437 -0.04768872  0.00744121 -0.02722271]\n"," [-0.01300015  0.03453108  0.04390124 -0.02796248 -0.01919132  0.0014916\n","   0.03302205  0.0110528   0.02355763  0.03311569]\n"," [-0.01300015  0.03453108  0.04390124 -0.02796248 -0.01919132  0.0014916\n","   0.03302205  0.0110528   0.02355763  0.03311569]\n"," [ 0.04141453 -0.03586144  0.04468853  0.00243197  0.0425258   0.02570314\n","   0.03169549 -0.00939401 -0.00173203  0.02032695]\n"," [ 0.00069119  0.0211736  -0.04973649  0.00702355 -0.03485416 -0.00299828\n","   0.02871437 -0.04768872  0.00744121 -0.02722271]], shape=(10, 10), dtype=float32)\n","tf.Tensor(\n","[[[ 0.02531358]\n","  [ 0.02799585]\n","  [-0.01580938]\n","  [ 0.01671995]\n","  [-0.02855491]\n","  [ 0.03134619]\n","  [-0.02139058]\n","  [-0.01615366]\n","  [-0.02857158]\n","  [-0.01546197]]\n","\n"," [[ 0.04141453]\n","  [-0.03586144]\n","  [ 0.04468853]\n","  [ 0.00243197]\n","  [ 0.0425258 ]\n","  [ 0.02570314]\n","  [ 0.03169549]\n","  [-0.00939401]\n","  [-0.00173203]\n","  [ 0.02032695]]\n","\n"," [[ 0.00069119]\n","  [ 0.0211736 ]\n","  [-0.04973649]\n","  [ 0.00702355]\n","  [-0.03485416]\n","  [-0.00299828]\n","  [ 0.02871437]\n","  [-0.04768872]\n","  [ 0.00744121]\n","  [-0.02722271]]\n","\n"," [[-0.01300015]\n","  [ 0.03453108]\n","  [ 0.04390124]\n","  [-0.02796248]\n","  [-0.01919132]\n","  [ 0.0014916 ]\n","  [ 0.03302205]\n","  [ 0.0110528 ]\n","  [ 0.02355763]\n","  [ 0.03311569]]\n","\n"," [[ 0.04141453]\n","  [-0.03586144]\n","  [ 0.04468853]\n","  [ 0.00243197]\n","  [ 0.0425258 ]\n","  [ 0.02570314]\n","  [ 0.03169549]\n","  [-0.00939401]\n","  [-0.00173203]\n","  [ 0.02032695]]\n","\n"," [[ 0.00069119]\n","  [ 0.0211736 ]\n","  [-0.04973649]\n","  [ 0.00702355]\n","  [-0.03485416]\n","  [-0.00299828]\n","  [ 0.02871437]\n","  [-0.04768872]\n","  [ 0.00744121]\n","  [-0.02722271]]\n","\n"," [[-0.01300015]\n","  [ 0.03453108]\n","  [ 0.04390124]\n","  [-0.02796248]\n","  [-0.01919132]\n","  [ 0.0014916 ]\n","  [ 0.03302205]\n","  [ 0.0110528 ]\n","  [ 0.02355763]\n","  [ 0.03311569]]\n","\n"," [[-0.01300015]\n","  [ 0.03453108]\n","  [ 0.04390124]\n","  [-0.02796248]\n","  [-0.01919132]\n","  [ 0.0014916 ]\n","  [ 0.03302205]\n","  [ 0.0110528 ]\n","  [ 0.02355763]\n","  [ 0.03311569]]\n","\n"," [[ 0.04141453]\n","  [-0.03586144]\n","  [ 0.04468853]\n","  [ 0.00243197]\n","  [ 0.0425258 ]\n","  [ 0.02570314]\n","  [ 0.03169549]\n","  [-0.00939401]\n","  [-0.00173203]\n","  [ 0.02032695]]\n","\n"," [[ 0.00069119]\n","  [ 0.0211736 ]\n","  [-0.04973649]\n","  [ 0.00702355]\n","  [-0.03485416]\n","  [-0.00299828]\n","  [ 0.02871437]\n","  [-0.04768872]\n","  [ 0.00744121]\n","  [-0.02722271]]], shape=(10, 10, 1), dtype=float32)\n","tf.Tensor(\n","[[-0.00245665]\n"," [ 0.01617989]\n"," [-0.00974564]\n"," [ 0.01205181]\n"," [ 0.01617989]\n"," [-0.00974564]\n"," [ 0.01205181]\n"," [ 0.01205181]\n"," [ 0.01617989]\n"," [-0.00974564]], shape=(10, 1), dtype=float32)\n"]}]},{"cell_type":"markdown","source":["## **Subwords**"],"metadata":{"id":"i23Rd93vxjcG"}},{"cell_type":"code","source":[""],"metadata":{"id":"TIh9Ucs2yQb3"},"execution_count":null,"outputs":[]}]}