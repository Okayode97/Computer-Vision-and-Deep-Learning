{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Udacity Intro to TensorFlow for Deep Learning, Lesson 10 Part 2.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMv87QlfpF2k9GUtvg2sBOJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Udacity: Intro to TensorFlow for Deep Learning**\n","## **Lesson 10 NLP: Recurrent Neural Networks**\n","\n","This lessons extends on what was covered in lesson 9. It introduces recurrent neural networks, which are able to capture temporal dependences,that change over time.\n","\n","This lesson would covers\n","- Different RNNs: Simple RNN, LSTMS, GRUs\n","- Text generation using NLP models.\n","\n","<br>\n","\n","I've split lesson 10 into 2 parts. This section covers text generation using NLP.\n","\n","<br>\n","\n","**Note**   \n","Part 1 of this lesson explored different recurrent neural networks and CNN 1D that can be trained on embeddings from a sequence. I've gone through the basic variations of RNNs (simpleRNN, LSTMs and GRU).\n"],"metadata":{"id":"I9uh67Za5iLL"}},{"cell_type":"markdown","source":["## **Text Generation**\n","\n","We can use the methods learnt so far with some slight modification to generate new text. Summary on text generation approach.   \n","\n","**1. Preapre text: Tokenize, sequence and create embedding**\n","  - Key difference: generate more samples from an individual sequence\n","  - From an individual sequence we can generate more sequences by increasing length untill we reach the maximum length of the sequence\n","  - The generated sequences would be padded to have the same length.\n","  - The last value in each generated sequence would be used as the label\n","  - one hot encode the labels\n","\n","```python\n","  # initial sequence \n","  [81, 82, 142, 197, 29, 4, 287, 197]\n","\n","  # generated sequences\n","  [81, 82]\n","  [81, 82, 142]\n","  [81, 82, 142, 197]\n","  [81, 82, 142, 197, 29]\n","  [81, 82, 142, 197, 29, 4]\n","  [81, 82, 142, 197, 29, 4, 287]\n","  [81, 82, 142, 197, 29, 4, 287, 197]\n","\n","  # padded sequences\n","  [0, 0, 0, 0, 0, 0, 81, 82]\n","  [0, 0, 0, 0, 0, 81, 82, 142]\n","  [0, 0, 0, 0, 81, 82, 142, 197]\n","  [0, 0, 0, 81, 82, 142, 197, 29]\n","  [0, 0, 81, 82, 142, 197, 29, 4]\n","  [0, 81, 82, 142, 197, 29, 4, 287]\n","  [81, 82, 142, 197, 29, 4, 287, 197]\n","\n","  # Split into training and label\n","  [0, 0, 0, 0, 0, 0, 81],  [82]\n","  [0, 0, 0, 0, 0, 81, 82], [142]\n","  [0, 0, 0, 0, 81, 82, 142], [197]\n","  [0, 0, 0, 81, 82, 142, 197], [29]\n","  [0, 0, 81, 82, 142, 197, 29], [4]\n","  [0, 81, 82, 142, 197, 29, 4], [287]\n","  [81, 82, 142, 197, 29, 4, 287], 197]\n","```\n","**2. Define model architecture, loss, metrics and activation**\n","  - Define a model to perform multi-class categorization\n","  - use the required activation function and define the expected number of possible classes at the last class\n","\n","**3. Train model**\n","- Train the final model on the padded generated sequence and one-hot encodded labels\n","\n","<br>\n","\n","**Notes**\n","- To create more variance in the generated text, as opposed to selecting the next most probable word in the distribution, the probability distribution can be used to define the odd of selecting a word.\n","\n","- After the model as been trained, we would define a seed sentence, from which we would generate new text from. The generated text from the seed sentence are then feed in recursively untill (i assume) the length of the sentence matches the max length of the sequence."],"metadata":{"id":"QE8_nhCT6j6P"}},{"cell_type":"markdown","source":["## **Text Generation in Code**"],"metadata":{"id":"tRbFhxCTT9fQ"}},{"cell_type":"markdown","source":["### **Import dependencies**"],"metadata":{"id":"XFc3JuWRUMOu"}},{"cell_type":"markdown","source":["### **Get and prepare text for NLP model**"],"metadata":{"id":"f9fbJ4DBUPOu"}},{"cell_type":"markdown","source":["### **Define Model**"],"metadata":{"id":"pryd9vv4UZqE"}},{"cell_type":"markdown","source":["### **Train Model**"],"metadata":{"id":"DuF6DwBcUhk9"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"zajEoP8r5E6N"},"outputs":[],"source":[""]},{"cell_type":"code","source":[""],"metadata":{"id":"en9m_BUU-l8_"},"execution_count":null,"outputs":[]}]}